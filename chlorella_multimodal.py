# ========================================================================
# SISTEMA MULTI-MODAL PARA CHLORELLA VULG EN FOTOBIORREACTORES
# ========================================================================
# Sistema para predicci√≥n de biomasa de Chlorella vulgaris
# con detecci√≥n autom√°tica de data leakage, validaci√≥n temporal y
# an√°lisis estad√≠stico
#
# 1. GESTI√ìN INTELIGENTE DE DATOS (SmartDataManager)
#    - Detecci√≥n autom√°tica de data leakage
#    - Umbral de correlaci√≥n (0.95)
#    - Validaci√≥n de variables biol√≥gicas
#    - Limpieza y preprocesamiento
#
# 2. INGENIER√çA DE CARACTER√çSTICAS BIOL√ìGICAS (BioFeatureEngine)
#    a) Variables Fotosint√©ticas
#       - Eficiencia lum√≠nica (Michaelis-Menten)
#       - Fotoinhibici√≥n
#       - Penetraci√≥n de luz
#    b) Variables Ambientales
#       - Efectos de temperatura y pH (funciones gaussianas)
#       - Estr√©s ambiental
#       - Interacciones multi-factor
#    c) Din√°mica de Nutrientes
#       - Limitaci√≥n por nutrientes (funci√≥n sigmoide)
#       - Efecto Monod
#    d) Variables Temporales
#       - Ciclos diurnos
#       - Fases de crecimiento
#
# 3. SELECCI√ìN DE CARACTER√çSTICAS (3 m√©todos)
#    - Correlaci√≥n de Pearson
#    - SelectKBest (f_regression)
#    - Random Forest (50 √°rboles)
#    ‚Üí Ensemble de los tres m√©todos
# 
# 4. DIVISI√ìN DE DATOS , REPRODUCIBILIDAD Y NORMALIZACI√ìN
#
#   - Divisi√≥n por escenarios (75% entrenamiento, 25% validaci√≥n)
#   - Semilla aleatoria fija (50)
#   - Normalizaci√≥n robusta (RobustScaler)
#   - Estandarizaci√≥n (StandardScaler)
#   
#
# 5. SISTEMA MULTI-MODELO
#    a) Modelos Base
#       - Regresi√≥n Lineal
#       - Ridge
#       - Random Forest
#    b) Modelos Avanzados
#       - PINN (Physics-Informed Neural Network)
#       - LSTM (Long Short-Term Memory)
#       - XGBoost
#    c) Ensemble Ponderado
#       - Pesos basados en rendimiento
#       - Validaci√≥n temporal
#
# 6. EVALUACI√ìN Y VALIDACI√ìN
#    - M√©tricas: R¬≤, RMSE, MAE, MAPE, NSE, Bias
#    - An√°lisis de residuos
#    - Detecci√≥n de overfitting
#    - Visualizaciones estad√≠sticas
#
# CARACTER√çSTICAS ANTI-OVERFITTING:
# - Validaci√≥n temporal (en la divisi√≥n por escenarios 45/15)
# - Detecci√≥n de data leakage
# - Ensemble de modelos
# - Regularizaci√≥n L2 (Ridge Regression y AdamW)


# =======================================================================
# inicio del c√≥digo:
# ========================================================================
# importo las librer√≠as necesarias:

# pandas: para manipulaci√≥n de datos
# numpy: para operaciones num√©ricas
# torch: para modelos de aprendizaje profundo
# sklearn: para preprocesamiento y m√©tricas
# xgboost: para modelos de boosting
# matplotlib y seaborn: para visualizaci√≥n
# datetime: para manejo de fechas
# warnings: para manejar advertencias
# scipy: para estad√≠sticas y pruebas
# scipy.stats: para pruebas estad√≠sticas avanzadas
# statsmodels: para herramientas estad√≠sticas avanzadas

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.model_selection import train_test_split
import xgboost as xgb
import matplotlib.pyplot as plt
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

print("CHLORELLA VULGARIS SISTEMA MULTI-MODAL")
# MUESTRO fecha y hora de inicio
#print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


# ========================================================================
# PASO 1: CARGA Y PROCESAMIENTO INTELIGENTE
# ========================================================================

#----------------------------------------------------------------------
# Concepto: data leakage (fuga de datos)
#----------------------------------------------------------------------
# El data leakage ocurre cuando el modelo tiene acceso a informaci√≥n
# que no deber√≠a tener durante el entrenamiento, lo que puede llevar
# a una sobreestimaci√≥n de su rendimiento. Por ejemplo, si el modelo
# tiene acceso a valores futuros que no estar√≠an disponibles en el
# momento de la predicci√≥n.

# Para evitar esto necesito:
# 1. Separar los datos de entrenamiento y prueba antes de cualquier preprocesamiento.
# 2. No incluir variables que contengan informaci√≥n futura o que sean
#    directamente derivadas del objetivo a predecir.
# 3. Validar que las variables de entrada no contengan informaci√≥n que
#    pueda filtrar el objetivo de manera directa.
#----------------------------------------------------------------------


# leakage_threshold es un umbral de correlaci√≥n que se usa para detectar
# data leakage o fuga de datos 

# la correlacion nos indica la fuerza de la relaci√≥n lineal entre dos variables
# si es cercana a 1, significa que una variable est√° directamente relacionada
# con la variable objetivo, lo que puede indicar fuga de datos.

# es decir, si una variable tiene una correlaci√≥n alta con la variable objetivo,
# es probable que est√© filtrando informaci√≥n que no deber√≠a estar disponible, 
# la variable podr√≠a estar utilizando informaci√≥n futura o derivada del objetivo 
# para hacer predicciones y eso es lo que queremos evitar.

class SmartDataManager:
    def __init__(self, leakage_threshold=0.95):

        # Inicializo el umbral de correlaci√≥n para detectar data leakage
        # normalmente se usa un valor alto, de forma arbitraria, de 0.95 y se usa porque,
        # como ya expliqu√©, correlaciones cercanas a 1 indican que la variable esta 
        # directamente relacionada con la variable objetivo (cosa a evitar)

        self.leakage_threshold = leakage_threshold

        # guardo el valor por defecto de leakage_threshold 0.95
        
    def carga_limpia_datos(self, filename='complete_dataset.csv'):
       # Carga, valida y procesa datos en una funci√≥n
        try:
            #creo un nuevo DataFrame vac√≠o (data)
            # pd.read_csv carga el dataset complete_dataset.csv y lo almacena en el DataFrame llamado data
            data = pd.read_csv(filename)

            print(f"‚úÖ Dataset: {data.shape}") #muestro el tama√±o del dataset (filas, columnas)

            # ----------------------------------------------------
                # Detectar data leakage autom√°ticamente
            # ----------------------------------------------------   

            leakage_cols = self._detecta_leakage(data)

            # leakage_cols son las columnas (variables) que pueden causar fuga de datos
            #-----------------
            # _detecta_leakage
            #-----------------
             # lo que hace esta funcion es analizar las columnas del dataframe y detectar las variables
             # que tienen una alta correlacion con la variable objetivo (biomasa)
             # se queda con las variables que superen el umbral de 0.95 establecido en el punto anterior
            
            if leakage_cols:
                print(f"üö´ Eliminando {len(leakage_cols)} caracter√≠sticas que producen fuga de datos")
                # En caso de detectar alguna variable que cause fuga de datos,
                # se imprime un mensaje indicando cu√°ntas variables se eliminar√°n
                data = data.drop(columns=leakage_cols)
                # y se eliminan las columnas que causan fuga de datos del nuevo DataFrame

            return data, leakage_cols
        # devuelvo el dataframe limpio sin las columnas que causan fuga de datos y tambi√©n 
        # devuelvo una lista de las columnas que causan fuga de datos

        # en caso de que no se encuentre el archivo
        except FileNotFoundError:
            print("‚ùå Dataset no encontrado. Aseg√∫rate de que el archivo 'complete_dataset.csv' est√© en el directorio correcto.")
            return None, []

    def _detecta_leakage(self, data):
       # funcion a√±adida para detectar data leakage
        # recibe un DataFrame y devuelve una lista de columnas que pueden causar fuga de datos


        # Verifico si la columna 'Biomass_g_L' est√° presente
        # ya que es la variable objetivo y si no est√°, no puedo detectar fuga de datos
        if 'Biomass_g_L' not in data.columns:
            return [] # devuelvo una lista vac√≠a si no est√° la columna 'Biomass_g_L'
        # asi en vez de lanzar un error, simplemente devuelvo una lista vac√≠a y el codigo sigue funcionando
        
        # asigno las variables que s√© seguro (por conocimiento biologico) que podr√≠an causar fuga de datos 
        # pues son las variables del dataFrame que se relacionan directamente con la biomasa

        conocidas = ['Cell_Concentration_cells_mL', 'Cell_Density_10E6_mL', 
                'Instantaneous_Productivity_g_L_d', 'Specific_Growth_Rate_h']
        # ahora realizo una seleccion inteligente de columnas num√©ricas del DataFrame
        cols_numericas = data.select_dtypes(include=[np.number]).columns.tolist()
        # data.select_dtypes() es un metodo de la libreria pandas que filtra las columnas de un DataFrame
        # en funcion del tipo de dato que contiene
        # incluye los enteros y decimales y excluye texto, fechas y booleanos
        # .columns devuelve los nombres de las columnas seleccionadas

        # finalmente me quedo con un indice con los nombres de todas las columnas num√©ricas
        # y calculo la correlaci√≥n entre las columnas num√©ricas y la columna 'Biomass_g_L'
        

        if len(cols_numericas) > 1:
            # en caso de que haya mas de una columna numerica
            corr = data[cols_numericas].corrwith(data['Biomass_g_L']).abs()
            # .corrwith(biomasa) calcula la correlacion de cada variable con la biomasa
            # .abs() devuelve el valor absoluto de la correlaci√≥n, para evitar valores negativos
            # se guardan los datos de las correlaciones en una lista llamada corr

            corr_alta = corr[corr > self.leakage_threshold].index.tolist()
            # para cada variable, en caso de que su correlaci√≥n con la biomasa sea mayor al umbral de 0.95
            # se guarda el nombre de la variable en una lista llamada corr_alta

            if 'Biomass_g_L' in corr_alta:
                corr_alta.remove('Biomass_g_L')
                # la biomasa siempre tiene correlacion 1 con si misma, 
                # con corr_alta.remove('Biomass_g_L') elimino la biomasa de mi ultima lista pues no es una variable que me influya al ser la variable objetivo
        else:
            corr_alta = []
            # si por algun casual ya no estaba en la lista devuelvo una lista vacia (error elegante en progr.)

        suspected = list(set(conocidas + corr_alta))
        # lo siguiente es juntar las variabes biologicas que ya s√© que pueden causar DL (conocidas) y las variables que superan el umbral de correlaci√≥n (corr_alta)
        # set() elimina duplicados que puedan aparecer en ambas listas y luego convierto a una nueva lista

        problematicas = [col for col in suspected if col in data.columns]

        # recorro columna a columna la lista de variables sospechosas
        # y compruebo que est√©n en el dataset original, luego las guardo en una nueva lista (problematicas)
        if problematicas:
            print(f"‚ö†Ô∏è Leakage detectado: {problematicas}")
        
        return problematicas
    
   
# ========================================================================
# PASO 2: FEATURE ENGINEERING BIOL√ìGICO
# ========================================================================

# El objetivo es crear un conjunto de datos con muchas variables que mejoren
# la capacidad predictiva del modelo, para que pueda aprender patrones complejos

#crear_features:
# Partiendo del Dataset (tabla con columnas temperatura, pH, luz, biomasa, etc)
# la idea es a√±adir nuevas columnas al dataset que representan caracter√≠sticas √∫tiles para predecir la biomasa
# (Biomass_g_L). Es como a√±adir nuevas formas de describir los datos para que el modelo
# entienda mejor c√≥mo se relacionan con el objetivo 
class BioFeatureEngine:
    def __init__(self): #inicializo la clase BioFeatureEngine

# Un outlier es un valor at√≠pico que puede aparecer en el conjunto de datos que se desv√≠a
# significativamente de otros valores en el conjunto de datos. 
# Los outliers pueden distorsionar los resultados del modelo
#--------------------------------------------------------------------
# RobustScaler para manejar outliers y normalizar
#-------------------------------------------------------------------
# RobustScaler es una t√©cnica de escalado que utiliza la mediana y el rango intercuart√≠lico
# para escalar las caracter√≠sticas, lo que lo hace robusto a los outliers
# es decir, transforma los datos para que tengan una media de 0 y una desviaci√≥n est√°ndar de 1
# Esto es √∫til para que los modelos de aprendizaje autom√°tico funcionen mejor
        
# self.scalers es un diccionario que contiene dos escaladores:
# features: son las variables de entrada que se utilizan para predecir el objetivo
# target: es la variable que se quiere predecir, en este caso Biomass_g_L

        self.scalers = {'features': RobustScaler(), 'target': StandardScaler()}
        # Inicializo un atributo para almacenar las features seleccionadas
        self.selected_features = None

#incorporo una copia del DataFrame data para trabajar sobre el sin modificar el original      
    def create_features(self, data):
        df = data.copy()
        print("üß¨ Creando features biol√≥gicas...")
        
        # Variables fotosint√©ticas O FEATURES a√±adidas para aportar informacion al modelo


        # ----------------------------------------------------
        # ECUACI√ìN de Michaelis-Menten:
        # ----------------------------------------------------

        # La eficiencia de la luz se calcula como la tasa de fotosintesis
        # en funci√≥n de la intensidad de luz (PAR) y una constante de saturacion
        
        # -------------------------
        # P = (I * Pmax) / (I + K)
        # -------------------------

        # P es la tasa de fotos√≠ntesis, con un valor normalizado entre 0 y 1
        # I es la intensidad de luz 
        # K es la constante de semisaturaci√≥n
        # Pmax es la tasa m√°xima de fotos√≠ntesis, siendo el valor maximo normalizado 1 

        # K representa la intensidad de luz donde la tasa de fotos√≠ntesis es la mitad de su valor m√°ximo, 150 ¬µmol/m¬≤/s

        # Falkowski, P. G., & Raven, J. A. (2013). Aquatic Photosynthesis. Princeton University Press.

        # Curvas P-I para microalgas y fitoplancton, obtuvo valores de entre 50 y 200 ¬µmol/m¬≤/s para especies comunes como Chlorella.
        #  El valor de 150 es un promedio comun para microalgas en cultivos controlados.

        # 2. Jassby-Platt para eficiencia lum√≠nica
        alpha_jp = 0.012  # Eficiencia inicial
        Pmax_jp = 1.0     # Tasa m√°xima normalizada
        df['eficiencia_luminica__jassby_platt'] = Pmax_jp * np.tanh((alpha_jp * df['PAR_umol_m2_s']) / Pmax_jp)

        #-----------------------------------------------------
        # ECUACION DE LA FOTOINHIBICI√ìN:
        #-----------------------------------------------------

        # La fotoinhibici√≥n es el da√±o a la fotos√≠ntesis causado por una luz excesiva
        # Se modela como una funci√≥n lineal de la intensidad de luz que excede un umbral
        # En este caso, se considera que la fotoinhibici√≥n comienza a ocurrir por encima de 300 ¬µmol/m¬≤/s
        # y aumenta linealmente hasta 400 ¬µmol/m¬≤/s, donde se considera m√°xima
        
        # -------------------------------------
        # F = ( PAR - 300) / 100 si PAR > 300
        # -------------------------------------


        # dividido por 100 para obtener un valor normalizado entre 0 y 1 de la fotoinhibici√≥n
        
        # F = 0 si PAR <= 300
        # por debajo de 300 no se produce fotoinhibici√≥n

        # Referencias:
        # Long, S. P., Humphries, S., & Falkowski, P. G. (1994). fotoinhibicion of photosynthesis in nature. Annual Review of Plant Biology, 45(1), 633-662.
        # - Menciona que microalgas como Chlorella pueden mostrar fotoinhibici√≥n a partir de 250-300 ¬µmol/m¬≤/s en cultivos densos.

        # Tredici, M. R. (2010). Photobiology of microalgae mass cultures: understanding the tools for the next green revolution. Biofuels, 1(1), 143-162
        # - Explica c√≥mo la fotoinhibici√≥n limita la productividad en cultivos de microalgas en fotobiorreactores, especialmente a intensidades de luz superiores a 200-400 ¬µmol/m¬≤/s.
        

        
        # np.maximum(0, x) compara x con 0 y devuelve el m√°ximo entre ambos 

        df['fotoinhibicion'] = np.maximum(0, (df['PAR_umol_m2_s'] - 300) / 100)
        
        # ------------------------------------------------------
        # EFECTOS DE TEMPERATURA Y pH
        # ------------------------------------------------------

        # Las funciones gaussianas modelan c√≥mo el crecimiento de microalgas (medido como Biomass_g_L) 
        # responde a la temperatura y el pH, con un pico en condiciones √≥ptimas y una ca√≠da sim√©trica
        # en condiciones sub√≥ptimas
        # es decir, la funci√≥n gaussiana me permite modelar c√≥mo cambia la biomasa en funci√≥n de la temperatura y el pH
        # La temperatura √≥ptima es 28¬∞C y el pH √≥ptimo es 8.0

        # -----------------------------------------
        # funci√≥n gaussiana:
        # temp_efecto = exp(-(temp - Œº)¬≤ / (2œÉ¬≤))
        # -----------------------------------------

        # donde Œº es el valor √≥ptimo (28¬∞C o 8.0 pH) y œÉ controla la amplitud de la curva
        # La amplitud determina cu√°n r√°pido disminuye la biomasa a medida que nos alejamos de la temper


        # muchas microalgas toleran desviaciones de 5-10¬∞C antes de un colapso significativo, 
        # en el caso de la temperatura he decidido darle una amplitud œÉ de 5, 
        # Esto significa que el crecimiento de microalgas es tolerante a desviaciones de ¬±5¬∞C
        # desde el √≥ptimo de 28¬∞C (rango 23-33¬∞C), con una reducci√≥n significativa del
        # crecimiento a ¬±10¬∞C

        # Raven & Geider (1988) indican que el crecimiento disminuye sim√©tricamente alrededor 
        # del √≥ptimo (~28¬∞C), con tolerancia a desviaciones de 5-10¬∞C

        df['efecto_temp'] = np.exp(-((df['Temperature_C'] - 28)**2) / 50)

        # El pH √≥ptimo para microalgas es 7.5-8.5, y desviaciones de ¬±0.5-1.0 unidades afectan 
        # la disponibilidad de carbono y el metabolismo. Una œÉ de 1 refleja esta alta sensibilidad

        # pH_efecto = exp(-(pH - Œº)¬≤ / (2œÉ¬≤))

        # Goldman & Azam (1978) muestran que desviaciones de ¬±0.5 unidades
        # desde pH 8.0 reducen significativamente la fotos√≠ntesis.

        df['efecto_pH'] = np.exp(-((df['pH'] - 8.0)**2) / 2)

        
        # ----------------------------------------------------
        # ESTRES AMBIENTAL:
        # ----------------------------------------------------

        # El estr√©s ambiental se calcula como la desviaci√≥n de la temperatura y pH √≥ptimos
        # La temperatura √≥ptima es 28¬∞C y el pH √≥ptimo es 8.0
        # Se mide como la distancia absoluta a estos valores √≥ptimos, normalizada
        

        # La diferencia absoluta (|Temperature_C - 28|) mide cu√°nto se aleja la temperatura
        # del punto ideal para el crecimiento de microorganismos fotosint√©ticos

        df['estres_de_temperatura'] = np.abs(df['Temperature_C'] - 28)

        # La diferencia absoluta (|pH - 8.0|) mide cu√°nto se aleja el pH del punto ideal

        df['estres_ph'] = np.abs(df['pH'] - 8.0)

        # La suma ponderada de ambos da una medida compuesta de estr√©s ambiental
        # asumo que el estr√©s por pH (/10) tiene un peso relativo cinco veces mayor que el estr√©s por temperatura (/2) esto lo corroborar√© por ahora lo dejar√© as√≠
        
        df['estres_ambiental'] = df['estres_de_temperatura']/10 + df['estres_ph']/2
        
         # referencias:
        # Eppley, R. W. (1972). Temperature and phytoplankton growth in the sea. Fishery Bulletin, 70(4), 1063-1085.
        # Relevancia: el crecimiento de las microalgas es menos sensible a la temperatura que a otros factores como el pH o la luz en rangos t√≠picos de cultivo.
        # - Menciona que la temperatura √≥ptima para microalgas como Chlorella es de 25-30¬∞C, y el pH √≥ptimo es de 7.5-8.5.

        # Raven, J. A., & Geider, R. J. (1988). Temperature and algal growth. New Phytologist, 110(4), 441-461.
        # -Indica que las microalgas toleran rangos de temperatura m√°s amplios (20-35¬∞C) que rangos de pH. Una desviaci√≥n de 5¬∞C del √≥ptimo reduce el crecimiento, pero no tanto como una desviaci√≥n de 0.5 unidades de pH.
        
        #Hinga, K. R. (2002). Effects of pH and temperature on phytoplankton physiology. Journal of Plankton Research, 24(12), 1201-1216.
        # Relevancia: Sugiere que la temperatura tiene un impacto m√°s gradual que el pH, justificando un factor de normalizaci√≥n menor (como 1/10)
        
        
        # ----------------------------------------------------
        # ECUACIONES DE DIN√ÅMICA DE NUTRIENTES:
        # ----------------------------------------------------

        if 'Nutrients_g_L' in df.columns:

            # 1. Limitaci√≥n por nutrientes (Funci√≥n sigmoide)
            # ----------------------------------------------
            # L = 1 / (1 + exp(5 * (N - 0.5)))
            # ----------------------------------------------
            # - N es la concentraci√≥n de nutrientes (g/L)
            # - 0.5 es el punto medio de la curva
            # - 5 es el factor que controla la pendiente
            #
            # La funci√≥n devuelve un valor entre 0 y 1:
            # - Cerca de 1 cuando hay pocos nutrientes (limitaci√≥n alta)
            # - Cerca de 0 cuando hay muchos nutrientes (limitaci√≥n baja)
            
            #df['nutrient_limitation'] = 1 / (1 + np.exp(5 * (df['Nutrients_g_L'] - 0.5)))
            
            # 2. Efecto de los nutrientes (Monod)
            # ----------------------------------
            # E = N / (N + Ks)
            # ----------------------------------
            # - N es la concentraci√≥n de nutrientes
            # - Ks = 0.02 es la constante de semisaturaci√≥n
            #
            # La funci√≥n modela el efecto de los nutrientes en el crecimiento:
            # - Tiende a 1 con alta concentraci√≥n de nutrientes
            # - Tiende a 0 cuando hay pocos nutrientes
            #
            # Referencias:
            # - Monod, J. (1949). The growth of bacterial cultures. Annual Review of Microbiology, 3(1), 371-394.
            # - Bernard, O. (2011). Hurdles and challenges for modelling and control of microalgae for CO2 mitigation 
            #   and biofuel production. Journal of Process Control, 21(10), 1378-1389.
            # 1. Haldane para nutrientes (incluye inhibici√≥n por exceso)
            Ks_haldane = 0.02  # Constante de semisaturaci√≥n
            Ki_haldane = 1.5   # Constante de inhibici√≥n
            df['efecto_de_nutrientes_haldane'] = df['Nutrients_g_L'] / (Ks_haldane + df['Nutrients_g_L'] + (df['Nutrients_g_L']**2 / Ki_haldane))
            
        
        # ----------------------------------------------------
        # VARIABLES CICLICAS Y DE FASES DE CRECIMIENTO
        # ----------------------------------------------------
        # con esto creo un reloj de 6h para capturar ciclos de dia y noche con la funci√≥n seno y coseno
        
        #El seno y el coseno convierten el tiempo en valores que "vuelven al inicio" cada 24 horas. 
        # Esto ayuda al modelo a entender que el tiempo es un ciclo continuo

        # 2œÄ es una constante matem√°tica (aproximadamente 6.28) que asegura que el ciclo se complete cada 24 horas. Dividir Time_h entre 24 normaliza el tiempo a un ciclo.
        # es importante usar ambos (seno y coseno) porque juntos capturan todas las posiciones del ciclo:
        # A las 0 horas (medianoche): sin_24h = 0, cos_24h = 1.
        # A las 6 horas: sin_24h = 1, cos_24h = 0.
        # A las 12 horas: sin_24h = 0, cos_24h = -1.
        # A las 18 horas: sin_24h = -1, cos_24h = 0.

        df['sin_24h'] = np.sin(2 * np.pi * df['Time_h'] / 24)
        df['cos_24h'] = np.cos(2 * np.pi * df['Time_h'] / 24)
        
        # Sin estas transformaciones, el modelo podr√≠a malinterpretar el tiempo 
        # (por ejemplo, pensar que las 23:00 del dia 1 est√°n muy lejos de las 00:00 del d√≠a 2)
        # es para que el modelo entienda que las 23:00 y las 00:00 son parte del mismo ciclo diario

        # Toma la columna Time_h (tiempo en horas) y la divide entre 24, usando una divisi√≥n entera (//), que redondea hacia abajo al n√∫mero entero m√°s cercano.
        # Esto convierte el tiempo en d√≠as completos. Por ejemplo:
        # Si Time_h = 25, entonces 25 // 24 = 1 (primer d√≠a).
        # Si Time_h = 50, entonces 50 // 24 = 2 (segundo d√≠a).
        # Si Time_h = 72, entonces 72 // 24 = 3 (tercer d√≠a).

        df['dia_actual'] = df['Time_h'] // 24
        
        # ------------------------------------------------------
        # INTERACCIONES AMBIENTALES
        # ------------------------------------------------------
       
        # En machine learning, a veces las variables individuales (como efecto_temp o efecto_pH)
        # no son suficientes para que el modelo entienda c√≥mo interact√∫an entre s√≠.
        # Multiplicar variables crea caracter√≠sticas derivadas que capturan interacciones
        # entre factores, lo que puede mejorar las predicciones

        # la primera interacci√≥n es entre la temperatura, el pH y la eficiencia de la luz
        
        df['capacidad_fotosintetica'] = df['efecto_temp'] * df['efecto_pH'] * df['eficiencia_luminica__jassby_platt']
        
        # Esta interacciones capturan c√≥mo la luz, la temperatura, la fotoinhibicion, el ph trabajan juntos para influir en la biomasa

        df['calidad_ambiental'] = df['capacidad_fotosintetica'] * (1 - df['fotoinhibicion'])
        

                              

        # ----------------------------------------------------
        # EFECTOS AVANZADOS DE CULTIVO: 
        # ----------------------------------------------------

       # if 'Culture_Age_h' in df.columns:

            # 1. Efecto de la densidad del cultivo
            # -------------------------------------
            # D = 1 / (1 + Œ± * t)
            # -------------------------------------
            # - t es la edad del cultivo en horas (Culture_Age_h)
            # - Œ± = 0.1 es el factor de atenuaci√≥n
            #
            # La funci√≥n modela c√≥mo el cultivo se vuelve m√°s denso con el tiempo:
            # - Comienza cerca de 1 (cultivo joven, poca densidad)
            # - Decrece gradualmente conforme envejece el cultivo
            # - Tiende asint√≥ticamente a 0 en cultivos muy viejos
            #
            # Referencias:
            # - Molina Grima, E., et al. (1994). Effect of growth rate on the eicosapentaenoic acid 
            #   and docosahexaenoic acid content of Isochrysis galbana in chemostat culture.

           # df['efecto_densidad_cultivo'] = 1 / (1 + 0.1 * df['Culture_Age_h'])

            # 2. Penetraci√≥n de luz efectiva
            # -----------------------------
            # P = E * D
            # -----------------------------
            # - E es la eficiencia lum√≠nica base
            # - D es el efecto de la densidad
            #
            # Combina la eficiencia lum√≠nica con el efecto de la densidad:
            # - Considera que la luz disponible disminuye en cultivos densos
            # - Modela el "self-shading" (auto-sombreado) del cultivo
            #
            # Referencias:
            # - Aci√©n Fern√°ndez, F.G., et al. (1997). A model for light distribution and average
            #   solar irradiance inside outdoor tubular photobioreactors for microalgal mass culture
           
           # df['penetracion_luminica'] = df['eficiencia_luminica'] * df['efecto_densidad_cultivo']
        
        # ------------------------------------------------------
        # ENCODING DE FASES DE CRECIMIENTO
        # ------------------------------------------------------

        # Parto de la columna Growth_Phase de mi dataset. Growth_phase indica la fase de crecimiento del cultivo
        # (por ejemplo, "decline", "linear", "stationary") y transformo cada fase en columnas num√©ricas
        # aplicando one-hot encoding, que convierte cada categor√≠a en una columna separada.
        # los modelos de machine learning no pueden trabajar directamente con texto (como "linear"), pero s√≠ con n√∫meros

        # pd.get_dummies crea columnas separadas para cada fase de crecimiento
        # Cada nueva columna tiene un valor de 1 si la fila corresponde a esa fase, o 0 si no
        # get_dummies es una funci√≥n de pandas que convierte variables categ√≥ricas en variables dummy (0 o 1)
        
        if 'Growth_Phase' in df.columns:
            phase_dummies = pd.get_dummies(df['Growth_Phase'], prefix='phase')
        # pd.concat combina el DataFrame original con las nuevas columnas de fases, las columnas se a√±aden horizontalmente
            df = pd.concat([df, phase_dummies], axis=1)
        
        #--------------------------------------------------------
        # LIMPIAR DATOS NO VALIDOS
        #--------------------------------------------------------

        # Reemplazar infinitos y NaN
        # Reemplazo valores infinitos y NaN con el m√©todo forward fill (ffill) para rellenar hacia adelante
        # df.replace([np.inf, -np.inf], np.nan) reemplaza los valores infinitos por NaN
        # fillna(method='ffill') rellena los NaN con el √∫ltimo valor v√°lido hacia adelante
        # fillna(0) rellena los NaN restantes con 0

        df = df.replace([np.inf, -np.inf], np.nan).fillna(0)

        #---------------------------------------------------------
        # CLIP OUTLIERS
        # ---------------------------------------------------------

        # Clipping para manejar outliers
        # El c√≥digo identifica y limita los valores extremos (outliers) en todas las columnas 
        # num√©ricas del DataFrame (df), excepto en la columna Biomass_g_L (que es el objetivo a 
        # predecir). Los outliers son valores inusualmente altos o bajos que pueden distorsionar
        # los modelos de machine learning. Utilizo una t√©cnica llamada clipping para 
        # asegurarse de que todos los valores est√©n dentro de un rango aceptable

        # El clipping es una t√©cnica que limita los valores extremos a un rango espec√≠fico
        # En este caso, se usa el percentil 1 y 99 para definir los l√≠mites

        # df.select_dtypes(include=[np.number]) selecciona todas las columnas num√©ricas del DataFrame
        # y excluye las columnas no num√©ricas

        cols_numericas = df.select_dtypes(include=[np.number]).columns

        #Itera sobre cada columna num√©rica y aplica clipping a todas menos a 'Biomass_g_L' que es la variable a predecir
        # Esto es importante porque quiero poder conservar los valores reales de la biomasa

        for col in cols_numericas:

            # Si la columna no es Biomass_g_L, aplica clipping

            if col != 'Biomass_g_L':

                # Calcula los cuantiles 1% (Q1) y 99% (Q3) de la columna.
                # Q1 = df[col].quantile(0.01)
                # Q3 = df[col].quantile(0.99)
                # Esto define un rango que incluye el 98% central de los datos, considerando el 1% m√°s bajo y el 1% m√°s alto como outliers
                # pues solo me interesa quitar los valores extremos que puedan afectar al modelo
                
                Q1, Q3 = df[col].quantile([0.005, 0.995])

                # df[col].clip(Q1, Q3) limita los valores de la columna al rango [Q1, Q3]

                df[col] = df[col].clip(Q1, Q3)

        print(f"‚úÖ Features Creados: {len(df.columns)} ")
        return df
    
# ========================================================================
# PASO 3: ENSEMBLE DE M√âTODOS DE SELECCI√ìN DE CARACTER√çSTICAS
# ========================================================================

# La combinaci√≥n de tres m√©todos diferentes (Correlaci√≥n + SelectKBest + Random Forest)
# proporciona una selecci√≥n de caracter√≠sticas m√°s robusta y confiable porque:
#
# 1. CADA M√âTODO DETECTA PATRONES DIFERENTES:
#    - Correlaci√≥n de Pearson: Detecta relaciones lineales directas
#    - SelectKBest (f_regression): Identifica relaciones estad√≠sticas m√°s complejas
#    - Random Forest: Captura interacciones no lineales y efectos combinados
#
# 2. COMPENSACI√ìN DE DEBILIDADES:
#    - Correlaci√≥n: Puede perder relaciones no lineales importantes
#    - SelectKBest: Puede pasar por alto interacciones complejas
#    - Random Forest: Puede ser sensible al ruido en los datos
#
# 3. VALIDACI√ìN CRUZADA IMPL√çCITA:
#    - Si una variable es seleccionada por m√∫ltiples m√©todos
#    - Mayor confianza en su importancia real, pues se estar√≠a seleccionando 
#       por su correlacion lineal con la biomasa, su importancia estadistica y su relevancia en un modelo de Random Forest
#    - Reduce el riesgo de sobreajuste a un √∫nico m√©todo
#    - Reduce la probabilidad de seleccionar features por casualidad
#
# 4. MAYOR ROBUSTEZ:
#    - Sistema m√°s resiliente a fallos
#    - Si un m√©todo falla, los otros dos pueden compensar
#    - Mejor generalizaci√≥n del modelo final 

# # referencia:
# Brownlee, J. (2020). How to Choose a Feature Selection Method For Machine Learning
# Jason Brownlee explica que existen tres grandes categor√≠as de t√©cnicas de selecci√≥n de caracter√≠sticas:
# Filter methods como la correlaci√≥n o SelectKBest, que eval√∫an cada variable respecto al objetivo de forma independiente.
# Wrapper methods y embedded methods como Random Forest, que utilizan algoritmos predictivos para evaluar la contribuci√≥n de cada caracter√≠stica
# a mayores,
# El art√≠culo de D. Huang et al. (2023) introduce un m√©todo de selecci√≥n de caracter√≠sticas basado en ensemble learning, 
# combinando varias t√©cnicas mediante ponderaciones optimizadas (validaci√≥n cruzada), 
# mejorando la precisi√≥n y robustez en modelos de predicci√≥n en series temporales

    def seleccion_y_preparacion_features(self, df, max_features=None):

        # Excluyo columnas que no son relevantes para la predicci√≥n de biomasa
        # Estas columnas seran importantes mas adelante para configurar la PINN y el LSTM
        
        _excluye_ = ['Biomass_g_L', 'Scenario', 'Growth_Phase', 'DateTime']
        
        # columnas_dataset crea una lista (columnas_dataset) con todas las columnas del DataFrame que no est√°n en la lista excluye
        # [c for c in df.columns if c not in excluye] recorre todas las columnas del DataFrame df y excluye las que est√°n en excluye
        # df[c].dtype in [np.number] asegura que solo se seleccionen columnas num√©ricas 

        columnas_dataset = [c for c in df.columns if c not in _excluye_ and df[c].dtype in [np.number]]
        
        # FILTRAR SOLO COLUMNAS NUM√âRICAS
        # df[columnas_dataset].select_dtypes(include=[np.number]) selecciona solo las columnas num√©ricas del DataFrame df
        # guardo las columnas num√©ricas en X_all

        X_all = df[columnas_dataset]

        # Aseguro que Biomass_g_L es la variable objetivo

        y = df['Biomass_g_L']
        
        # ------------------------------------------------------------------------
        # CORRELACION DE PEARSON
        # ------------------------------------------------------------------------

        # La correlaci√≥n (correlaci√≥n de Pearson) mide la relaci√≥n lineal entre cada columna de mi dataframe 
        # (con los featues a√±adidos) y la variable objetivo (biomasa)

        # Un valor cercano a 1 o -1 indica una relaci√≥n fuerte, mientras que un valor cercano a 0
        #  indica poca relaci√≥n.
        
        # X_all.corrwith(y).abs() calcula la correlaci√≥n absoluta para priorizar caracter√≠sticas
        # con relaciones fuertes, independientemente de si son positivas o negativas

        # sort_values(ascending=False) ordena las caracter√≠sticas por su correlaci√≥n absoluta, de mayor a menor
        
        # esto me permite establecer un primer filtro para poder seleccionar las caracter√≠sticas
        # m√°s relevantes que puedan influir a la hora de la prediccion de la biomasa
        
        correlations = X_all.corrwith(y).abs().sort_values(ascending=False)

        # correlaciones.head(n_features_max) selecciona las primeras 20 caracter√≠sticas con mayor correlaci√≥n
        # index.tolist() convierte los √≠ndices de las correlaciones en una lista

        correlation_threshold = correlations.median()  # 0.2908
        top_corr = correlations[correlations > correlation_threshold].index.tolist()



        # X_all.columns.tolist()[:n_features_max] obtiene una lista de todas las columnas de X_all
        # en caso de error, selecciona las primeras 20 columnas

        # ------------------------------------------------------------------------
        # SCIKIT LEARN (SelectKBest)
        # ------------------------------------------------------------------------

        # SelectKBest es una clase con la que se puede seleccionar las mejores caracter√≠sticas
        # de un conjunto de datos para usar en el entrenamiento de modelos de aprendizaje 
        # supervisado basado en su importancia estad√≠stica. 
        # 
        # Eval√∫a la importancia de cada variable para predecir la biomasa
        # Para ello se le debe indicar una m√©trica de rendimiento, en mi caso f_regresion, con la que calcula
        # la correlacion entre cada caracter√≠stica y la biomasa. Despu√©s, convierte la correlaci√≥n en un F-score. 
        # F-score es una metrica estadistica que mide la significancia de la relacion entre la varaible y la biomasa.
        #  Mayor F-score = Mayor importancia estad√≠stica.

        # Una vez obtenida la puntuaci√≥n selecciona √∫nicamente la K mejores del conjunto de datos.
        # As√≠, para poder usar esta clase, es necesario seleccionar dos hiperpar√°metros:
        #  la funci√≥n de m√©trica (f_regression) y el valor de K = max_features como n√∫mero de caracter√≠sticas a seleccionar

        # LA puntuacion de F-score no se mide como la de pearson (-1 a 1),
        # sino que es un valor positivo, cuanto mayor sea el F-score, m√°s relevante es
        #    - F-score = 0: Variable no importante
        #    - F-score bajo (1-10): Importancia d√©bil
        #    - F-score medio (10-100): Importancia moderada
        #    - F-score alto (>100): Importancia fuerte

        try:

            # f_regression es una funci√≥n de Scikit-learn que calcula la correlaci√≥n entre 
            # cada caracter√≠stica y la variable objetivo

            # Se usa para seleccionar las caracter√≠sticas m√°s relevantes para la regresi√≥n
            # Crea un objeto SelectKBest de Scikit-learn, una herramienta para seleccionar las k mejores 
            # caracter√≠sticas seg√∫n la m√©trica f_regression).

            selector = SelectKBest(f_regression, k='all')
            

            # Ajusta el modelo SelectKBest a los datos, x_all es el dataframe con todas las caracter√≠sticas e 'y' es la variable objetivo biomasa
            
            
            selector.fit(X_all, y)
            f_scores = selector.scores_
            f_threshold = np.median(f_scores)  # Mediana de F-scores
            selected_mask = f_scores > f_threshold
            
            # selector.get_support() devuelve un booleano indicando qu√© columnas 
            # fueron seleccionadas (las k con los puntajes F m√°s altos).

            # X_all.columns[...] extrae los nombres de estas columnas.
            # .tolist() convierte los nombres en una lista (top_f), por ejemplo, ['Cell_Density_10E6_mL', 'temp_pH_synergy', 'pH']
          
            top_stats = X_all.columns[selected_mask].tolist()

            # top_stats contiene la lista de las mejores caracter√≠sticas seleccionadas por SelectKBest

        except:
            # En caso de error, se usa la lista de correlaciones como respaldo
            top_stats = top_corr
       
        # Referencias:
        # - Guyon, I., & Elisseeff, A. (2003). An introduction to variable and
        #   feature selection. Journal of Machine Learning Research, 3, 1157-1182.

        # ========================================================================
        # RANDOM FOREST
        # ========================================================================
        
        # En esta secci√≥n uso Random Forest para identificar las variables m√°s 
        # relevantes para predecir la biomasa, complementando los m√©todos
        # de correlaci√≥n y SelectKBest

        # Random Forest utiliza un ensemble de 50 (arbitrario) √°rboles de decisi√≥n para identificar las variables
        # m√°s relevantes para predecir la biomasa. Cada √°rbol analiza diferentes
        # subconjuntos de datos y caracter√≠sticas, proporcionando una medida robusta de importancia
      

        # Cada arbol de decision tiene un nodo raiz inicial.
        # El nodo raiz contiene muestra aleatoria de datos de entrenamiento y es el punto de partida para construir cada √°rbol
        # Un SPLIT (division) es el proceso de dividir los datos en un nodo de un √°rbol en dos o m√°s subnodos, 
        # bas√°ndose en una √∫nica caracter√≠stica (feature) y un valor de corte (threshold). 
        # El objetivo del split es crear subgrupos de datos m√°s homog√©neos con respecto a la variable objetivo

        # La idea de las divisiones y la creaci√≥n de subnodos es poder llegar a obtener una divisi√≥n de los datos en subgrupos de la forma m√°s eficiente posible,
        # de manera que cada subgrupo sea lo m√°s homog√©neo posible en relaci√≥n a la variable objetivo (Biomass_g_L)

        # una vez conseguido esto se puede calcular la importancia de cada caracter√≠stica
        # en funci√≥n de cu√°nto reducen los splits, cada split usa una √∫nica caracter√≠stica (feature), el error del modelo
        # es decir, la importancia de cada feature no se determina solo por la cantidad de splits sino por la calidad (cuanto reducen el error en la prediccion)

        # de otra forma:
        #  El algoritmo eval√∫a un subconjunto aleatorio de caracter√≠sticas en cada nodo para encontrar el mejor split
        # La importancia de cada caracter√≠stica se calcula tras construir el √°rbol, seg√∫n cu√°nto contribuyen los splits
        # que la involucran a reducir el error.

        # 1. SENSIBILIDAD AL RUIDO:
        #    - Cada √°rbol se entrena con una muestra aleatoria de datos (bootstrap)
        #    - Si hay ruido en los datos, algunos √°rboles pueden aprender de este ruido,
        #     pues pueden llegar a crear splits muy espec√≠ficos que se ajusten a los patrones aleatorios o ruido.
        #    - En datos biol√≥gicos, el ruido puede venir de:
        #      * Errores de medici√≥n en sensores
        #      * Variabilidad natural en el crecimiento
        #      * Fluctuaciones ambientales no controladas
        
        # 2. IMPORTANCIA DE CARACTER√çSTICAS:
        #    - Calcula importancia basada en cu√°nto mejora cada split
        #    - Variables ruidosas pueden parecer importantes por casualidad
        #    - Necesita m√∫ltiples √°rboles para promediar y reducir este efecto, 50 est√° bien para esto
        
        # 3. COMPENSACI√ìN:
        #    - Usar 50 √°rboles ayuda a reducir el impacto del ruido
        #    - Cada √°rbol ve una muestra diferente de datos
        #    - El promedio de muchos √°rboles es m√°s robusto
        
        # Por esto es importante combinarlo con otros m√©todos (correlaci√≥n y SelectKBest)
        # que son menos sensibles al ruido.
       
        try:
             # 1. Crear y entrenar modelo Random Forest
            # ---------------------------------------
            # - n_estimators=50: usa 50 √°rboles de decisi√≥n (es eficiente y relativamente r√°pido, m√°s de 50 da resultados parecidos pero con mayor procesamiento y tiempo)
            # - random_state=42: para reproducibilidad de resultados
           
            rf = RandomForestRegressor(n_estimators=50, random_state=42)
            rf.fit(X_all, y)
            
            # 2. Extraer importancia de caracter√≠sticas
            # ---------------------------------------
            # Crea una Serie de pandas donde:
            # - rf.feature_importances_: array con la importancia de cada variable
            # - index=X_all.columns: nombres de las variables como √≠ndices
            
            
            importances = rf.feature_importances_
            importance_threshold = np.median(importances)  # Mediana de importancias

            # 3. Seleccionar top features
            # ---------------------------------------
             # - nlargest(): selecciona las max_features variables m√°s importantes
             # - index.tolist(): convierte los nombres de variables en lista
            top_rf = X_all.columns[importances > importance_threshold].tolist()

        except:
            # Si falla el Random Forest, usa las correlaciones como plan B
            # Esto asegura que el sistema siga funcionando incluso si hay errores
            top_rf = top_corr
        
        # ========================================================================
        # COMBINAR M√âTODOS (CORRELACI√ìN + SELECTKBEST + RANDOM FOREST)
        # ========================================================================

        # Aqu√≠ combino las caracter√≠sticas seleccionadas por correlaci√≥n, SelectKBest y Random Forest
        # la idea es que con esta sumatoria de metodos, concateno en una sola lista las mejores caracteristicas de las 3, hasta el l√≠mite max_features
        # combino los 3 m√©todos de selecci√≥n de caracter√≠sticas (top_corr, top_stats y top_rf), 
        # aseguro el n√∫mero m√≠nimo de caracter√≠sticas, 
        # y creo el DataFrame final X para el entrenamiento
        
        self.selected_features = list(set(top_corr + top_stats + top_rf))[:max_features]
       
        # - set(): elimina duplicados
        # - [:max_features]: limita el n√∫mero total de features

        # - X_all[self.selected_features]: selecciona las columnas del DataFrame original que est√°n en self.selected_features
        X = X_all[self.selected_features]

        print(f"Variables en X_all: {len(X_all.columns)}")
        print(f"Variables en correlations: {len(correlations)}")
        print(f"max_features real: {max_features}")
        print(f"top_corr length: {len(top_corr)}")
        print(f"top_stats length: {len(top_stats)}")  
        print(f"top_rf length: {len(top_rf)}")
        print(f"Variables con correlaci√≥n 0: {(correlations == 0).sum()}")

        print(f"üéØ Features seleccionadas {len(self.selected_features)} ")






        #Create a bar plot of feature importance (if available from Random Forest)
        if top_rf:
            rf = RandomForestRegressor(n_estimators=50, random_state=42)
            rf.fit(X_all, y)
            importance_df = pd.DataFrame({
            'Feature': X_all.columns,
            'Importance': rf.feature_importances_
        })
        importance_df = importance_df[importance_df['Feature'].isin(self.selected_features)]
        importance_df = importance_df.sort_values('Importance', ascending=False)

        plt.figure(figsize=(10, 6))
        plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
        plt.xlabel('Importancia')
        plt.title('Importancia de las Features Seleccionadas (Random Forest)')
        plt.gca().invert_yaxis()  # Invert y-axis for better readability
        plt.tight_layout()
        plt.show()





        print(f"üìä Top 18: {self.selected_features[:18]}")
        
        # ------------------------------------------------------
        # DIVISI√ìN DE DATOS (train-test split)  
        # ------------------------------------------------------
        
        # data leakage: fuga de datos 
        # el data leakage ocurre cuando el modelo tiene acceso a informaci√≥n que no deber√≠a tener durante el entrenamiento,
        # lo que puede llevar a una sobreestimaci√≥n de su rendimiento.

        # en mi dataset, tengo una columna llamada Scenario, que va de 1 hasta 60, esta columna indica el cultivo al que pertenece cada fila
        # mi data set es muy completo porque contemplo 60 escenarios diferentes (cultivos), cada uno con sus propias condiciones ambientales y de cultivo
        # por eso, para evitar el data leakage, divido los datos en entrenamiento y prueba pero respetando el cultivo (Scenario) al que pertenecen.
        # de esta forma, el modelo solo ve datos de ciertos cultivos durante el entrenamiento y se prueba en cultivos diferentes que no ha visto antes
        #  75% para entrenamiento y 25% para testeo

        # con 25% de los cultivos para testeo o validaci√≥n, me refiero a la validaci√≥n final del modelo, es decir, guardar√© estos datos para evaluar el rendimiento del modelo despu√©s de entrenarlo

        # del 75% que uso para entrenamiento, usar√© una proporcion del 80-20 para entrenar pesos y para validar internamente durante el entrenamiento.

        # contin√∫o:
        # yo necesito que la prediccion sea continua en el tiempo, es decir,
        # me interesa respetar la secuencia temporal para poder simular la realidad, 
        # pues uso datos pasados para predecir el futuro, por eso es importante asumir cada escenario como
        # un bloque continuo en el tiempo

        # me interesa agrupar los datos por escenarios, de lo contrario si mezclo todos los datos
        # y luego realizo el entrenamiento con los datos mezclados, el modelo podr√≠a aprender patrones
        # que no son representativos de la realidad, ya que los datos de entrenamiento y prueba
        # estar√≠an mezclados y el modelo podr√≠a "ver" informaci√≥n futura durante el entrenamiento. 

        # referencia: 
        # Dado que los datos de crecimiento de las microalgas proceden de cultivos (escenarios) independientes con m√∫ltiples 
        # mediciones temporales por cultivo, estos no cumplen la suposici√≥n de ser independientes e id√©nticamente distribuidos, 
        # (Sasse et al. (2025)), en tales casos un split aleatorio entre filas puede provocar que muestras 
        # dependientes de un mismo cultivo queden repartidas en entrenamiento y prueba, generando data leakage y predicciones 
        # artificialmente elevadas. 
        # Por eso, se justifica separar los cultivos completos (es decir, los Scenario) entre los conjuntos de 
        # entrenamiento y evaluaci√≥n para estimar correctamente la capacidad de generalizaci√≥n real del modelo

        # Sasse, L., et al. (2025). Overview of leakage scenarios in supervised machine learning. Journal of Big Data


        # Lo primero es verificar que tengo la columna 'Scenario' para divisi√≥n en bloques de cultivo
        # 'Scenario' es una columna que indica el escenario (cultivo) al que pertenece cada fila

        if 'Scenario' in df.columns:
            # escenarios disponibles (cultivos)
            scenarios = np.array(sorted(df['Scenario'].unique()))

            # sorted ordena los escenarios de menor a mayor
            # df['Scenario'].unique() obtiene una lista de los cultivos √∫nicos (sin duplicados)
            # np.array convierte la lista en un array de numpy para facilitar operaciones posteriores
            # es decir, con esto obtengo una lista ordenada de los escenarios, ordeno los cultivos de menor a mayor, posiciono el n√∫mero de cultivos de 1-60 de forma ordenada


        # ---------------------------------------------- 
        # REPRODUCIBILIDAD Y SELECCION ALEATORIA
        # ----------------------------------------------

        # La reproducibilidad es la capacidad de obtener resultados consistentes e id√©nticos 
        # cuando se repite un experimento bajo las mismas condiciones. En ciencia de datos 
        # y machine learning, significa que cualquier persona debe poder ejecutar el c√≥digo y
        # obtener exactamente los mismos resultados.
        # Para garantizar la reproducibilidad, se establece una semilla aleatoria (np.random.seed(n))
        # Esto asegura que las operaciones aleatorias (como la divisi√≥n de datos) produzcan los mismos resultados en cada ejecuci√≥n.
            
            SEED = 50                      # semilla fija para reproducibilidad
            np.random.seed(SEED)           # fija la semilla global para NumPy
            
            # np.random.seed() genera una secuencia de n√∫meros pseudoaleatorios (a traves de un algoritmo)
            # con esta secuencia fija, las operaciones aleatorias producir√°n los mismos resultados en cada ejecuci√≥n del c√≥digo


        # La idea es que siempre se seleccionen 15 cultivos (de forma aleatoria), de los 60 totales, para validacion.
        # La clave de esto es que en cada ejecuci√≥n del codigo, los 15 cultivos, se seleccionen de forma aleatoria

        # de esta forma, el modelo se entrena siempre con 45 cultivos y se valida con 15 cambiandose los cultivos de validacion en cada ejecucion, de manera aleatoria
        # esto ayuda a evitar sesgos y asegura que el modelo generalice bien a diferentes cultivos
        # es decir, que no se sobreajuste a un conjunto fijo de cultivos de validacion

        # ===================================================================================================================================
        # NOTA IMPORTANTE:
        # en la version inicial del c√≥digo, usaba los primeros 45 cultivos para entrenamiento y los √∫ltimos 15 para validaci√≥n, siempre.
        # Al implementar esta parte, la selecci√≥n aleatoria de escenarios para validaci√≥n, he mejorado notablemente la capacidad de mi modelo predictivo,
        # pasando de un R2 de 0.85 a un R2 de 0.93, lo que indica una mejor generalizaci√≥n y precisi√≥n en las predicciones

        # esta parte es muy  importante porque a√±ade REPRESENTATIVIDAD: con selecci√≥n aleatoria (aunque fijada por seed), 
        # el conjunto de validaci√≥n suele ser m√°s parecido al global, y el modelo rinde mucho, pero mucho mejor.
        # =====================================================================================================================================
        
            n_total = len(scenarios) # total de escenarios (60)
            n_val = max(1, n_total // 4)   # divido el total de escenarios entre 4 (25%), resultado 15 (m√≠nimo 1 escenario)
            validacion_scenarios = np.random.choice(scenarios, size=n_val, replace=False)
            # np.random.choice es una funci√≥n de NumPy que sirve para seleccionar elementos de un conjunto de datos de forma aleatoria,
            # en el fondo se rige por la semilla que he fijado antes (50), por lo que la selecci√≥n ser√° siempre la misma en cada ejecuci√≥n del c√≥digo
            # 15 cultivos seleccionados para validaci√≥n de forma pseudoaleatoria pero siempre los mismos en cada ejecuci√≥n del c√≥digo.

            # estos escenarios se usar√°n para validaci√≥n (test)

            
            entrenamiento_scenarios = ~df['Scenario'].isin(validacion_scenarios)
            # isin() devuelve un booleano indicando si cada fila pertenece a los escenarios de validaci√≥n
            # ~ invierte el booleano, es decir, TRUE para filas que NO est√°n en los √∫ltimos 15 escenarios (es decir, los otros 45 escenarios)
            # estos son los que se usar√°n para entrenamiento

            X_entrenamiento, X_validacion = X[entrenamiento_scenarios], X[~entrenamiento_scenarios]
            # X_entrenamiento representa las features (entradas o predictores) de las filas que pertenecen a los primeros 45 escenarios (entrenamiento)
            # X_validacion representa las features (entradas o predictores) de las filas que pertenecen a los √∫ltimos 15 escenarios
            Y_entrenamiento, Y_validacion = y[entrenamiento_scenarios], y[~entrenamiento_scenarios]
            # Y_entrenamiento representa la biomasa (objetivo) de las filas que pertenecen a los primeros 45 escenarios (entrenamiento)
            # Y_validacion representa la biomasa (objetivo) de las filas que pertenecen a los √∫ltimos 15 escenarios (prueba o validacion)
            
            print(f"üöÇ Division de datos: Entrenamiento {len(X_entrenamiento)}, Validacion {len(X_validacion)}")


        else:
            X_entrenamiento, X_validacion, Y_entrenamiento, Y_validacion = train_test_split(X, y, test_size=0.25, random_state=42)
            # en caso de no tener la columna Scenario, hago un split aleatorio normal (25% validacion, 75% entrenamiento)
        
        #  ------------------------------------------------------
        # NORMALIZACI√ìN DE DATOS (StandardScaler)
        #  ------------------------------------------------------

        # self.scalers es un diccionario que contiene los objetos de normalizaci√≥n
            # self.scalers['features'] se usa para normalizar las caracter√≠sticas (X_entrenamiento, X_validacion)
            # self.scalers = {
                    #'features': StandardScaler(),  # EScalador estandar las caracter√≠sticas X
                    #'target': StandardScaler()     # Escalador estandar ara la variable objetivo y
            #}
            # fit_transform ajusta el escalador a los datos de entrenamiento y transforma los datos al mismo tiempo
            
            
            # X_entrenamiento_scaled es un DataFrame que contiene las caracter√≠sticas de entrenamiento normalizadas
            # pd dataframe convierte el array de numpy devuelto por fit_transform en un DataFrame de pandas
            # con las mismas columnas y el mismo √≠ndice que X_entrenamiento
            # self.scalers['features'] es un objeto StandardScaler de Scikit-learn que normaliza las caracter√≠sticas
            # transform aplica la normalizaci√≥n a los datos de prueba (X_entrenamiento) 
            # colums= X_entrenamiento.columns asegura que las columnas del DataFrame resultante sean solo las de X_entrenamiento
            # index=X_entrenamiento.index asegura que el √≠ndice del DataFrame resultante sea el mismo que X_entrenamiento

        X_entrenamiento_scaled = pd.DataFrame(
            self.scalers['features'].fit_transform(X_entrenamiento), 
            columns=X_entrenamiento.columns, index=X_entrenamiento.index
        ) # hago lo mismo con los datos de validacion:
        X_validacion_scaled = pd.DataFrame(
            self.scalers['features'].transform(X_validacion), 
            columns=X_validacion.columns, index=X_validacion.index
        )
        # aqui si que es necesario trabajar con el historico de datos de la biomasa pues es necesario
        #  entrenar y validar los modelos con la variable que se quiere predecir


         # Normalizo la variable objetivo (y_train, y_test) usando el mismo escalador
            # fit_transform es un metodo de standerdscaler que ajusta los datos de entrenamiento de la variable objetivo
            # devuelve un array de numpy con los datos normalizados
            # y_train.values.reshape(-1, 1) convierte la serie de pandas en un array de numpy de una sola columna
            # (-1, 1) asegura que sea un array bidimensional con una sola columna
            # transform aplica la normalizaci√≥n a los datos de prueba (y_test)
            # flatten() convierte el array de numpy en un array unidimensional
            # esto significa que los datos de la variable objetivo se normalizan
            # Y_entrenamiento_scaled es un array de numpy con los datos normalizados de la variable objetivo
            # quedando por ejemplo: [0.1, 0.2, 0.3, ...]
            
        Y_entrenamiento_scaled = self.scalers['target'].fit_transform(Y_entrenamiento.values.reshape(-1, 1)).flatten()
        Y_validacion_scaled = self.scalers['target'].transform(Y_validacion.values.reshape(-1, 1)).flatten()
        
        return X_entrenamiento_scaled, X_validacion_scaled, Y_entrenamiento_scaled, Y_validacion_scaled, Y_entrenamiento, Y_validacion

# ===================================================================================================
# PASO 5 : SISTEMA MULTI-MODELO (modelos predictivos y algoritmo de combinaci√≥n)
# ===================================================================================================
#    Modelos:
#       - PINN (Physics-Informed Neural Network)
#       - LSTM (Long Short-Term Memory)
#       - Regresi√≥n Lineal
#       - Ridge
#       - Random Forest
#       - XGBoost
#    Ensemble Ponderado
#       - Pesos basados en rendimiento
#       - Validaci√≥n temporal

# ========================================================================
#  RED NEURONAL INFORMADA POR RESTRICCIONES BIOL√ìGICAS (PINN)
# ========================================================================

# Esto es una red neuronal artificial de tipo perceptr√≥n multicapa que incorpora
# conocimiento biol√≥gico en su funci√≥n de p√©rdida para predecir la biomasa de microalgas
# en funci√≥n de variables ambientales y de cultivo
#  
# La arquitectura se organiza en varias capas densas intercaladas con funciones de activaci√≥n no lineales 
# y con t√©cnicas de regularizaci√≥n mediante dropout, lo que permite capturar relaciones complejas entre las 
# variables de entrada y al mismo tiempo reducir el sobreajuste. 
# 
# La primera capa incluye una normalizaci√≥n por lotes, que garantiza la homogeneidad de las variables en cada 
# iteraci√≥n de entrenamiento y facilita la convergencia del modelo.
# 
#  La salida de la red corresponde a un √∫nico valor continuo que representa la biomasa estimada en el cultivo.
#  La particularidad de este modelo est√° en la funci√≥n de p√©rdida, que combina el error cuadr√°tico medio con
#  un t√©rmino de penalizaci√≥n aplicado cuando la predicci√≥n resulta negativa. 
# 
# Biol√≥gicamente no es posible una biomasa menor que cero, entonces, esta formulaci√≥n introduce en el aprendizaje una restricci√≥n 
# coherente con el conocimiento de la fisica que regula a esta variable, convirtiendo al modelo en un ejemplo de red neuronal informada
# por la f√≠sica o por la biolog√≠a, lo que asegura que las predicciones sean no solo precisas, 
# sino tambi√©n consistentes con la realidad del sistema estudiado.


# ------------------------------------
# FUNCIONAMIENTO DE LA RED NEURONAL
# ------------------------------------
# Es un modelo matematico que se construye con neuronas artificiales, peque√±as unidades de calculo
# que reciben datos de entradas, los combinan con unos pesos, suma un sesgo, y pasa el resultado por
# una funci√≥n que decide cu√°nto de esa se√±al se transmite hacia adelante (capas).

# Con la uni√≥n de varias capas se forma la red neuronal. Cada capa transforma los datos un poco m√°s y 
# al final, la salida es un n√∫mero o una etiqueta: en este caso, la biomasa predicha.

# Cuantas m√°s neuronas haya, mayor capacidad tendr√° la red para aprender relaciones complejas, pero si pongo demasiadas
# es m√°s probable que memorice los datos en lugar de generalizar. Por eso 64-32 es un t√©rmino medio ampliamente utilizado 
# y es el que utilizo para este modelo.

# NORMALIZACI√ìN POR LOTES:
# BatchNorm  es una funci√≥n que normaliza cada feature (caracteristica) dentro de un lote de entrenamiento (batch). 
# genera un vector de features normalizados con media 0 y desviaci√≥n 1. Una vez obtengo este vector ya noramlizado, 
# entra en la primera capa lineal Linear(input_size,64), donde se calculan las primeras combinaciones 


# ReLU: Rectified Linear Unit, es una funci√≥n de activaci√≥n que se utiliza en cada una de las capas,
# despu√©s de que cada neurona artificial combine variables con pesos.
# Esto es necesario para introducir la NO LINEALIDAD, permitiendo que la red aprenda realciones complejas 
# y no s√≥lo combinaciones lineales con pesos.

# f(x)=max(0,x)

# esta funcion lo que hace es dejar pasar los valores positivos de X y convertir en 0 los valores negativos de X
# por ejemplo, aplicando la ecuacion a mi modelo, en la primera capa, una neurona aprende una combinaci√≥n de las variables de esta forma:

# h = 0.5‚ãÖ* pH ‚àí 0.2‚ãÖ*nutrientes + 0.1‚ãÖ*tiempo

# Ese h puede salir negativo, por ejemplo si los nutrientes son muy altos y pH bajo.

# Con ReLU, si h=‚àí3.2, la salida de la neurona ser√° 0.
# Si h=2.5, la salida ser√° 2.5

# As√≠, ReLU act√∫a como un filtro: ignora combinaciones no √∫tiles (cuando salen negativas) y deja pasar las √∫tiles.

# En cada capa, durante el entrenamiento se produce adem√°s una t√©cnica de regularizaci√≥n llamada dropout,
# que lo que hace es apagar aleatoriamente un porcentaje de neuronas en una capa.

# Dropout(0.3), significa que en cada paso de entrenamiento, en promedio el 30% de esas neuronas no participan, 
# y solo el 70% restante sigue activo.

# El objetivo del dropout es evitar el sobreajuste, obligando a que el aprendizaje se distribuya entre todas y no dependa √∫nicamente 
# de unas pocas neuronas concretas que den siempre la respuesta. 

# Es importante porque la red debe aprender patrones robustos que no dependan de unas pocas neuronas en espec√≠fico, 
# sino que todas tienen que poder aportar algo.  As√≠, nunca se sabe cu√°les estar√°n apagadas en la siguiente iteraci√≥n, 
# forzando a que la red siga funcionando bien aunque varias neuronas no est√©n disponibles.

# Repartir el conocimiento aporta una representaci√≥n mucho m√°s robusta del cultivo.


class CompactPINN(nn.Module): # Primero declaro una clase de red neuronal tipo MLP informada por restricciones biol√≥gicas
   
    def __init__(self, input_size): # Constructor: recibe el n√∫mero de caracter√≠sticas de entrada (columnas/features)
        super().__init__()          # Inicializo la superclase nn.Module
        self.net = nn.Sequential(   # Defino la arquitectura como una secuencia de capas
            nn.BatchNorm1d(input_size), # Normaliza por lotes cada feature para estabilizar y acelerar el entrenamiento
            nn.Linear(input_size, 64), nn.ReLU(), nn.Dropout(0.3), # Capa densa a 64 neuronas + ReLU + Dropout 30% para regularizar
            nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.2), # Segunda capa densa a 32 neuronas + ReLU + Dropout 20%
            nn.Linear(32, 1)                               # Capa de salida escalar: predice biomasa (valor continuo)
        )
    

    # ---------------------------------------------------------
    # FUNCION FORWARD
    # ---------------------------------------------------------
    # En PyTorch, una red neuronal define un m√©todo forward(x).
    # Aqu√≠ es donde se describe el procedimiento que hacen los datos
    # a trav√©s de las capas del modelo hasta producir una salida.
    #
    # En este caso, self.net contiene toda la secuencia de capas:
    #   1. BatchNorm ‚Üí normaliza los valores de entrada (pH, nutrientes, tiempo, etc.)
    #   2. Linear + ReLU + Dropout ‚Üí combinaciones no lineales y regularizaci√≥n
    #   3. Linear + ReLU + Dropout ‚Üí segunda transformaci√≥n intermedia
    #   4. Linear final ‚Üí genera una predicci√≥n escalar (biomasa)
    #
    # Al llamar al modelo con un lote de datos X (ejemplo: 32 filas x 20 features),
    # este forward se encarga de pasarlos por todas esas capas en orden.
    #
    # El resultado final tiene forma [batch_size, 1], es decir, una predicci√≥n de biomasa
    # para cada fila del lote de entrada.

    def forward(self, x): # x es un tensor de PyTorch con las caracter√≠sticas de entrada
        return self.net(x)  # Pasa los datos de entrada x a trav√©s de la red definida en self.net
    

     # ------------------------------------
     # Funcion de perdida personalizada
     # ------------------------------------

     # El objetivo de esta funci√≥n es calcular cu√°nto se equivoca 
     # la red en sus predicciones e imponer una restricci√≥n biol√≥gica:
     # la biomasa no puede ser negativa.

     # mse = nn.MSELoss()(pred.squeeze(), target)

     # Calcula el error cuadr√°tico medio entre las predicciones del modelo (pred) y los 
     # valores reales de biomasa (target).
     # "squeeze()" elimina dimensiones sobrantes, establece la misma dimension de vector tanto para pred como para target.

     #  bio_penalty = torch.mean(torch.relu(-pred)) * 5

     # Aplico ReLU a (-pred), convirtiendo en positivos los valores de predicci√≥n que sean negativos,
     # luego calculo la media de estos "errores biologicos" y por √∫ltimo multiplico x5 para auumentar la penalizaci√≥n.

     # return mse + 0.1 * bio_penalty

     # El error total es el error cuadr√°tico medio sumado a una penalizaci√≥n del 10% cuando la red intenta predecir valores de biomasa negativos.

     # ***nota: ********
     # se podr√≠a mejorar esta parte, cambiando la fijaci√≥n manual de penalizaci√≥n (5 para la dureza de la penalizaci√≥n y 0.1 para el peso relativo) 
     # por un sistema de pesos adaptativos en funci√≥n de cu√°nto se equivoque en la prediccion,
     # para que el sistema aprenda la ponderaci√≥n entre MSE y penalizaci√≥n durante el entrenamiento. Un multiplicador de Lagrange podr√≠a estar bien.

     # justifico un x5 en dureza de la penalizaci√≥n y un 0.1 para su peso relativo, porque al probar con otros valores 
     # (2 10 15) o (0.1 0.3 0.4) el modelo es un poco peor.


    def bio_loss(self, pred, target): 
        mse = nn.MSELoss()(pred.squeeze(), target)
        bio_penalty = torch.mean(torch.relu(-pred)) * 5  # Penalizaci√≥n por predicciones negativas
        return mse + 0.1 * bio_penalty # Peso relativo de la penalizaci√≥n sumada al MSE

# =======================================================================
# RED NEURONAL LSTM (Long Short-Term Memory)
# =======================================================================

# CompactLSTM es una clase que define una red neuronal basada en LTSM (Long Short-Term Memory)
# Esta red est√° dise√±ada para capturar dependencias temporales en los datos (series temporales).
# De esta forma, la idea es que el modelo pueda "recordar" c√≥mo evoluciona la biomasa en el tiempo en funci√≥n de las condiciones del cultivo.
# Debe aprovechar la informaci√≥n de los pasos previos para mejorar la predicc√≥n en el momento actual.

# Importante: La LSTM ya est√° dise√±ada para manejar series temporales, yo s√≥lo le doy la secuencia ordenada en el tiempo,
# y ella se encarga de recordar c√≥mo estaban las variables en pasos anteriores y combinarlas con el presente.

# definicion de la capa LSTM:

# self.lstm = nn.LSTM(input_size, 32, batch_first=True, dropout=0.2)

# La LSTM recibe como entrada secuencias con el tama√±o (input_size) 
# Se configuran 32 unidades ocultas (hidden units), una unidad  oculta es un neuri√≥n artificial donde se realizan todas 
# las operaciones en cada capa, con esto defino la dimensionalidad interna.

# batch_first=True significa que los tensores de entrada a la LSTM se organizan como [n√∫mero de muestras, longitud de la secuencia, n√∫mero de variables]
# esto es lo mas comun a la hora de crear una LSTM.

# Se aplica un dropout=0.2 para mejorar la generalizaci√≥n y reducir el riesgo de sobreajuste.


class CompactLSTM(nn.Module):
    def __init__(self, input_size):
        super().__init__()         # Inicializo la superclase nn.Module, que es la base de todas las redes neuronales en PyTorch
        self.lstm = nn.LSTM(input_size, 32, batch_first=True, dropout=0.2) # Capa LSTM con 32 unidades ocultas y dropout del 20%
       
       
       # Definici√≥n de la red de salida o capa densa (fully connected)
        # ---------------------------------------------------------------
        # La salida de la LSTM en el √∫ltimo paso temporal (dimensi√≥n 32)
        # se procesa por una peque√±a red densa:
        # - Linear(32 -> 16): capa oculta intermedia con 16 neuronas
        # - ReLU(): funci√≥n de activaci√≥n no lineal que introduce capacidad
        #   de aprendizaje de relaciones complejas
        # - Dropout(0.2): regularizaci√≥n adicional
        # - Linear(16 -> 1): capa final que predice la biomasa (un valor escalar)

        self.out = nn.Sequential(nn.Linear(32, 16), nn.ReLU(), nn.Dropout(0.2), nn.Linear(16, 1)) # Capas densas para procesar la salida de la LSTM y generar la predicci√≥n final
   
    # forward funciona igual que en el caso anterior

    def forward(self, x): # x es un tensor de PyTorch con las caracter√≠sticas de entrada
        # x tiene forma [batch_size, sequence_length, input_size].
        # La LSTM procesa toda la secuencia y devuelve:
        # - lstm_out: representaciones ocultas de cada paso temporal
        # - _: (hidden_state, cell_state) que no se utilizan aqu√≠
        lstm_out, _ = self.lstm(x) # Paso los datos de entrada x a trav√©s de la capa LSTM

        return self.out(lstm_out[:, -1, :]) # Uso solo la salida del √∫ltimo paso temporal para la predicci√≥n final
        # Selecciono la salida correspondiente al √∫ltimo paso temporal
        # (lstm_out[:, -1, :]) ya que es la que resume toda la secuencia previa.
        # Esto es porque, en el √∫ltimo paso, la LSTM ha procesado toda la secuencia y su vector oculto contiene 
        # una codificaci√≥n comprimida de la historia completa.

        # Esta representaci√≥n comprimida se pasa a la red fully connected (capa densa), produciendo la predicci√≥n final de biomasa.

# ========================================================================
# MODELO COMBINADO(combina varios modelos y pondera por rendimiento)
# ========================================================================

# El modelo combinado se basa en una idea sencilla: combinar varios modelos diferentes para obtener un resultado final 
# m√°s preciso y estable que cualquiera de ellos por separado.

# La idea es que cada modelo tiene sus propias fortalezas y debilidades, y al combinarlos,
# se pueden compensar los errores individuales y aprovechar las ventajas de cada uno.

# He contemplado 3 enfoques diferentes:
# - modelos lineales (Linear y Ridge)
# - modelos de √°rboles de decisi√≥n (Random Forest y XGBoost)
# - redes neuronales (PINN y LSTM)

# Cada uno de estos enfoques tiene caracter√≠sticas distintas que pueden ser √∫tiles para predecir la biomasa:
# Los modelos lineales sirven para encontrar tendencias generales, relaciones lineales, como por ejemplo si al aumentar los nutrientes
#  tambi√©n aumenta la biomasa de forma proporcional.

# Los modelos de arbol de decision no buscan una relaci√≥n directa o proporcional sino que dividen los datos en distintas condiciones,
# permitiendo descubrir interacciones m√°s complejas, como que el efecto de los nutrientes en la biomasa depende de un rango espec√≠fico de pH 
# (explicado en la secci√≥n de RandomForest)

# XGBoost es parecido pero con un enfoque m√°s avanzado: va corrigiendo los errores de otros √°rboles anteriores y ajust√°ndose cada vez m√°s a los datos.

# Con la PINN le "ense√±o" al modelo que no puede predecir valores negativos de biomasa, porque eso no tiene un sentido real.
# Y la LSTM, est√° pensada para trabajar con informaci√≥n que cambia en el tiempo, para que pueda aprender de c√≥mo han ido cambiando los cultivos a lo largo del tiempo y tener en cuenta esta evoluci√≥n.

# una vez definidos los modelos, se entrenan todos con los mismos datos de entrenamiento.
# aplicando validaci√≥n cruzada interna (hold-out), utilizo una proporci√≥n 80-20 dentro del bloque de netrenamiento.
# es decir, de los 45 cultivos que uso para entrenamiento,  36 cultivos son para entrenar pesos y 9 cultivos para validar internamente.

# criterio adaptativo: antes de hacer la combinaci√≥n de todos los modelos, cada modelo se eval√∫a en un conjunto de validaci√≥n (los 15 cultivos restantes), 
# que son datos que no han visto durante el entrenamiento. En ese conjunto se mide su error con el error cuadr√°tico medio (MSE). 

# Calculo el error (MSE) de cada modelo, y calculo su inverso:

# calculo el inverso (1/Ei) del error, obtengo los valores proporcionales a la calidad de cada modelo cuanto menor es el error, mayor es el valor.
# el problema aqui es que si utilizo los valores resultantes directamente como pesos, el resultado final no estar√≠a en una escala definida y no habr√≠a forma de interpretar correctamente qu√© aporta cada modelo realmente.

# Por eso realizo una normalizaci√≥n, que consiste en dividir cada valor inverso por la suma total de los inversos:

# peso_n = (1/Ei) / ( ‚àë_{i=1}^{M} 1/Ei)

# As√≠, saco los pesos reales normalizados de cada modelo:

# Pred_final = (peso_1 * pred_A) + (peso_2 * pred_B) + ... + (peso_n * pred_m)

# De esta forma consigo que mi modelo combinado se asegure de que los mejores modelos tengan m√°s voz en la decisi√≥n y
# que los peores apenas influyan.

class CompactMultiModel:
    def __init__(self):
        # Diccionario para almacenar los modelos entrenados
        self.models = {}
        self.val_scores = {}
        
    def train_all(self, X_entrenamiento, Y_entrenamiento, epochs=150):
        # Funci√≥n para entrenar todos los modelos y calcular sus pesos basados en el rendimiento de validaci√≥n
        print("\nüöÄ Entrenando modelos")
        
        # ------------------------------------------------------------------
        # 1) VALIDACI√ìN INTERNA (hold-out): 80% train / 20% validaci√≥n
        # ------------------------------------------------------------------

        # - Este split es *solo* dentro del bloque de ENTRENAMIENTO (es decir,
        #   NO toca los 15 escenarios de validaci√≥n final que guard√© aparte).
        # - Sirve para: (a) ajustar las RNA (paradas tempranas sencillas) y
        #   (b) medir el error de cada modelo y derivar pesos del ensemble.
        
        X_tr, X_val, y_tr, y_val = train_test_split(X_entrenamiento, Y_entrenamiento, test_size=0.2, random_state=42)

        # test_size=0.2 significa que el 20% de los datos se usan para validaci√≥n interna
        # random_state=42 fija la semilla para que la divisi√≥n sea reproducible
        
        # ------------------------------------------------------------------
        # 2) ENTRENAMIENTO DE MODELOS 
        # ------------------------------------------------------------------
        # Aqu√≠ defino y entreno varios modelos base con diferentes enfoques:    
        #   * Lineal/Ridge: relaciones proporcionales y efecto de regularizaci√≥n.
        #   * RandomForest: no lineal, maneja interacciones y umbrales.

        self.models['Linear'] = LinearRegression().fit(X_tr, y_tr)

        # -----------------------------------------------------
        # -----------------------------------------------------
        # Regularizacion L2 en Ridge Regression
        # -----------------------------------------------------
        # -----------------------------------------------------
        # *Regresion Ridge:
        # A la hora de entrenar un modelo, lo que se hace es ajustar los coeficientes para minimizar el error entre lo que se predice y lo real.
        # Uno de los problemas que pueden aparecer es, si mis datos tienen muchas variables o incluso se correlacionan mucho entre s√≠, que los coeficientes 
        # podr√≠an llegar a crecer de forma desproporcionada. Esto produce que el modelo se sobreajuste: funciona bien en entrenamiento pero
        # muy mal en validaci√≥n o con nuevos datos.

        # La regularizaci√≥n lo que hace es a√±adir un t√©rmino extra de 'penalizaci√≥n' a la funci√≥n de coste, para castigas los modelos demasiado complejos
        # En el caso de Ridge Regression (y AdamW m√°s adelante) incorporan una regularizaci√≥n L2, cuya particularidad es que,
        # en este caso (regularizacion L2), el castigo o la penalizaci√≥n es la suma de los cuadrados de los coeficientes.
        # Dicho de otra forma, es un moderador, o regualdor, de pesos espec√≠fico que mitiga el overfitting.

        self.models['Ridge'] = Ridge(alpha=1.0).fit(X_tr, y_tr)

        # *RandomForest:
        # RandomForest es un conjunto de √°rboles de decisi√≥n que dividen los datos en funci√≥n de condiciones espec√≠ficas.
        # Por ejemplo, un √°rbol podr√≠a aprender que si el pH > 7.5 y los nutrientes > 50, entonces la biomasa es alta.
        # Otro √°rbol podr√≠a aprender que si el pH < 6.5 y la temperatura < 20¬∞C, entonces la biomasa es baja.
        # Al combinar muchos √°rboles (100 en este caso), el modelo puede capturar interacciones complejas entre las variables.
        # max_depth=8 limita la profundidad de cada √°rbol para evitar sobreajuste.
        # random_state=42 fija la semilla para reproducibilidad.
        self.models['RandomForest'] = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42).fit(X_tr, y_tr)
    
        
        # *XGBoost:
        # XGBoost es una implementaci√≥n avanzada de boosting que crea √°rboles secuenciales,
        # donde cada nuevo √°rbol intenta corregir los errores de los anteriores.
        # Esto permite capturar patrones complejos y mejorar la precisi√≥n.
        # Par√°metros como learning_rate=0.03 controlan la velocidad de aprendizaje,
        # subsample=0.8 y colsample_bytree=0.8 introducen aleatoriedad para mejorar la generalizaci√≥n.
        # reg_alpha y reg_lambda son t√©rminos de regularizaci√≥n para evitar sobreajuste.
        # random_state=42 asegura reproducibilidad.
        
        self.models['XGBoost'] = xgb.XGBRegressor(
            n_estimators=300, max_depth=6, learning_rate=0.03,
            subsample=0.8, colsample_bytree=0.8,
            reg_alpha=0.1, reg_lambda=1.0, random_state=42
        ).fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)
        
        # ------------------------------------------------------------------
        # 4) Redes neuronales: PINN y LSTM
        # ------------------------------------------------------------------
        # Las redes en PyTorch trabajan con tensores (no con DataFrames de pandas).
        # Por eso tengo que convertir X (features) e y (objetivo) desde pandas ‚Üí tensores float32.
        X_tr_t, y_tr_t = torch.FloatTensor(X_tr.values), torch.FloatTensor(y_tr) # X_tr_t = [n_trains,n_features] frente a y_tr_t = [n_trains]
        X_val_t, y_val_t = torch.FloatTensor(X_val.values), torch.FloatTensor(y_val)
        
        # ---------------------------------
        # PINN
        # ---------------------------------

        # creo una instancia del modelo PINN definido antes.
        # X_entrenamiento.shape[1] devuelve el n¬∫ de variables de entrada (features)

        # el n¬∫ de features pasa al constructor del modelo para definir el tama√±o de la capa de entrada:

        #  self.models['PINN'] = CompactPINN(X_entrenamiento.shape[1])

        # Ahora defino el optimizador para entrenar la PINN.:
        # un optimizador es un m√©todo de descenso por gradiente que ajusta los pesos de la red para que las predicciones se acerquen lo m√°ximo posible a los datos reales.
        # Adam es un optimizador tiene la caracter√≠stica de que se adapta autom√°ticamente a cada peso de la red calculando promedios del gradiente pasado y del cuadrado del gradiente.
        # Es mejor que otros optimizadores b√°sicos como SGD, pero a d√≠a de hoy se utiliza un optimizador que funciona mucho mejor, llamado AdamW.

        # AdamW corrige varios errores de la versi√≥n Adam original y se ha convertido en el est√°ndar en deep learning.
        # Con AdamW consigo que la red neuronal aprenda r√°pido, en un sistema con muchas features, que sea estable y que tenga regularizaci√≥n L2 para evitar el sobreajuste.
        # As√≠, utilizo: AdamW con lr=0.001 y weight_decay=0.01 (regularizacion L2), que son valores est√°ndar y seguros.

        self.models['PINN'] = CompactPINN(X_entrenamiento.shape[1])
        opt_pinn = torch.optim.AdamW(self.models['PINN'].parameters(), lr=0.001, weight_decay=0.01)
        

        # Guardo el mejor error de validaci√≥n encontrado hasta ahora.
        # float('inf') es "infinito positivo", un valor inicial muy alto.
        # As√≠ cualquier p√©rdida real calculada ser√° menor y podr√° reemplazarlo.
        best_pinn = float('inf')

        # En ML y deep learning, una √©poca (epochs) significa un recorrido completo por todos los datos de entrenamiento.
        # En la pr√°ctica no se suelen pasar todas las filas de golpe, ser√≠a poco eficiente. Lo que se hace es dividir
        # los datos en lotes m√°s peque√±os llamados batches.
        # As√≠, dentro de una '√©poca', el modelo procesa lote por lote, calcula el error, ajusta un poco los pesos, y pasa al siguiente lote, hasta completar as√≠ todo el dataset.
        # De esta manera, cuando termina de ver todas las filas una vez se dice que ha completado una √©poca

        # Una vez s√© esto, entro en el bucle de entrenamiento por epochs (√©pocas), 
        # que son las repeticiones completas sobre los datos de entrenamiento.
        # √âste es un valor arbitrario, yo decido cu√°ntas veces quiero que el modelo repase los datos.
        # Los valores de referencia suelen estar entre 100-200, he escogido 150.

        for epoch in range(epochs): # Desde 1 hasta 150 bucle
            self.models['PINN'].train() # Activa el modo "entrenamiento" de la red PINN.
            opt_pinn.zero_grad() # El optimizador (AdamW en este caso) guarda los gradientes de cada iteraci√≥n.
            # (we typically want to explicitly set the gradients to zero before starting to do backpropagation) *est√° epxlicado en el blog de machine learning*
            pred = self.models['PINN'](X_tr_t)# Forward pass: el modelo recibe los datos de entrenamiento (X_tr_t)
            # y genera predicciones de biomasa. Aqu√≠ todav√≠a no hay ajuste, solo predice con los pesos actuales.
            loss = self.models['PINN'].bio_loss(pred, y_tr_t) # Calcula la funci√≥n de p√©rdida, en este caso la "bio_loss",
            # que combina el error cuadr√°tico medio (MSE) con una penalizaci√≥n biol√≥gica (no permitir biomasa negativa).
            # Cuanto mayor sea la diferencia entre predicci√≥n y valores reales, mayor ser√° la p√©rdida.
            loss.backward() # Backpropagation: a partir de la p√©rdida, PyTorch calcula los gradientes
            # (derivadas parciales) de cada peso de la red respecto a ese error.
            # Esto es lo que le permite saber c√≥mo ajustar los pesos.
            torch.nn.utils.clip_grad_norm_(self.models['PINN'].parameters(), 1.0) # Gradient clipping: limita el tama√±o m√°ximo de los gradientes a 1.0.
            # Evita el problema de "exploding gradients", donde los valores se vuelven enormes y desestabilizan el entrenamiento.
            opt_pinn.step() # Paso del optimizador: AdamW utiliza los gradientes calculados en .backward()
            # para actualizar los pesos de la red y reducir la p√©rdida.
            # Este es el momento real en que la red "aprende" ajustando sus par√°metros.
            
            # -----------------------------------------------
            # Early selection
            # -----------------------------------------------

            # Se trata de una t√©cnica de regularizaci√≥n para evitar el sobreajuste.
            # Cada 50 √©pocas hago una pausa para evaluar el modelo en el conjunto de validaci√≥n.
            # El objetivo no es entrenar (no se actualizan pesos), sino comprobar si el modelo
            # realmente est√° mejorando en datos NO vistos durante el entrenamiento.
            # Al finalizar todas las √©pocas, selecciono el mejor punto y no me quedo s√≥lo con los pesos de la √∫ltima √©poca.
     
            if epoch % 50 == 0: # cada 50 epochs
                self.models['PINN'].eval() # Modo evaluaci√≥n: desactiva Dropout y fija BatchNorm (usa estad√≠sticas acumuladas).
              # As√≠ mido el rendimiento "real" sin ruido de regularizaci√≥n.

                with torch.no_grad():
                    # Bloque sin gradientes: m√°s r√°pido y ahorra memoria (no es necesario entrenar aqu√≠).
                    val_pred = self.models['PINN'](X_val_t)
                    val_loss = nn.MSELoss()(val_pred.squeeze(), y_val_t) # M√©trica de validaci√≥n: MSE sobre el conjunto de validaci√≥n interno (20% de train).
                    # Esta p√©rdida no ajusta pesos, solo la usamos para monitorizar y seleccionar el mejor modelo.

                    if val_loss < best_pinn:
                        # Early model selection: si mejora la p√©rdida de validaci√≥n,
                        # guardo una copia de los pesos actuales como "mejores hasta ahora".
                        best_pinn = val_loss
                        best_state = self.models['PINN'].state_dict().copy()
                     
                print(f"   PINN Epoch {epoch}: {val_loss.item():.4f}")
        
        self.models['PINN'].load_state_dict(best_state)
         # la idea de esto es quedarme con los mejores pesos de la red despues de pasar por todas las epochs.
         # Eso significa que el modelo final no son los pesos de la √∫ltima √©poca, sino los pesos de la √©poca que mejor funcion√≥ en validaci√≥n
        
        # -----------------------------------------LSTM------------------------------------------------------------------------------------------------------------------------------------
        
        self.models['LSTM'] = CompactLSTM(X_entrenamiento.shape[1])
        
        # Creao el modelo LSTM indicando cu√°ntas features hay por paso temporal.

        opt_lstm = torch.optim.AdamW(self.models['LSTM'].parameters(), lr=0.001, weight_decay=0.01)
        
        # Optimizador AdamW tambi√©n para el LSTM (buen rendimiento)
        # HIPERPAR√ÅMETROS SELECCIONADOS:
        # - lr=0.001: Learning rate (tasa de aprendizaje)
        #   * Valor conservador que balancea velocidad de convergencia vs. estabilidad
        #   * Para LSTMs, valores t√≠picos est√°n en el rango [0.0001, 0.01]
        # - weight_decay=0.01: Coeficiente de regularizaci√≥n L2
        #   * Penaliza pesos grandes para prevenir overfitting
        
        X_tr_seq, X_val_seq = X_tr_t.unsqueeze(1), X_val_t.unsqueeze(1)
        # Las redes LSTM esperan tensores con dimensionalidad espec√≠fica:
        # INPUT SHAPE REQUERIDO: (batch_size, sequence_length, input_features)
        # TRANSFORMACI√ìN APLICADA:
        # - Datos originales: (N_samples, N_features) ‚Üí Tensor 2D
        # - Despu√©s de unsqueeze(1): (N_samples, 1, N_features) ‚Üí Tensor 3D
        # - unsqueeze(1) a√±ade una dimensi√≥n en la posici√≥n 1 (sequence_length = 1)
        # - Esto simula secuencias de longitud unitaria para cada muestra
        best_lstm = float('inf') # variable que inicializo con infinito para que cualquier loss inicial sea considerado como mejora.

        # Hago lo mismo que en la red neuronal anterior:
        # bucle de entrenamiento basado en epochs
        for epoch in range(epochs):
            self.models['LSTM'].train() #Activa el modo de entrenamiento del modelo
            opt_lstm.zero_grad() # PyTorch acumula gradientes por defecto en .grad attributes
            # Necesario limpiar gradientes de la iteraci√≥n anterior
            pred = self.models['LSTM'](X_tr_seq).squeeze()# Forward pass: el modelo recibe los datos de entrenamiento (X_tr_t)
            # y genera predicciones de biomasa. Aqu√≠ todav√≠a no hay ajuste, solo predice con los pesos actuales.
            loss = nn.MSELoss()(pred, y_tr_t) # Funci√≥n de p√©rdida: Error Cuadr√°tico Medio entre predicci√≥n y objetivo.
            # # En la LSTM utilizo MSE "puro" (no la bio_loss), porque aqu√≠ no hay que imponer la restricci√≥n de no-negatividad
            # (esa restricci√≥n est√° s√≥lo  en la PINN).

            # Porque MSE y no otro parametro como MAE o Huber Loss?
            # porque me da la gana la verdad, adem√°s:

            #  PROPIEDADES DEL MSE:
            # - Diferenciable en todo punto 
            # - Penaliza cuadr√°ticamente los errores grandes
            # - Unidades: cuadrado de las unidades de la variable objetivo
            # - Sensible a outliers debido a la penalizaci√≥n cuadr√°tica

            loss.backward() # Backpropagation: calcula los gradientes de la p√©rdida con respecto a TODOS los par√°metros entrenables de la LSTM.
            # EXPLICACI√ìN TE√ìRICA:
            # PROBLEMA: Exploding Gradients en RNNs
            # - Los gradientes pueden crecer exponencialmente durante BPTT
            # - Esto causa inestabilidad num√©rica y divergencia del entrenamiento
            # - Especialmente problem√°tico en secuencias largas
            # La soluci√≥n? -> Clipping de Gradiente

            torch.nn.utils.clip_grad_norm_(self.models['LSTM'].parameters(), 1.0) # Clipping de gradiente (norma L2) a 1.0 para estabilizar el entrenamiento.
            # Evita "exploding gradients" t√≠picos en RNN/LSTM cuando hay dependencia temporal larga.

            opt_lstm.step()
             # Paso de optimizaci√≥n: AdamW actualiza los pesos usando los gradientes calculados.


            # -----------------------------------------------
            # Early selection
            # -----------------------------------------------

            # Se trata de una t√©cnica de regularizaci√≥n para evitar el sobreajuste.
            # Cada 50 √©pocas hago una pausa para evaluar el modelo en el conjunto de validaci√≥n.
            # El objetivo no es entrenar (no se actualizan pesos), sino comprobar si el modelo
            # realmente est√° mejorando en datos NO vistos durante el entrenamiento.
            # Al finalizar todas las √©pocas, selecciono el mejor punto y no me quedo s√≥lo con los pesos de la √∫ltima √©poca.
     

            if epoch % 50 == 0:
                 # Cada 50 √©pocas hago una evaluaci√≥n r√°pida en el conjunto de validaci√≥n para monitorizar overfitting
                 # y guardar el mejor estado de los pesos (early selection).
                 
                self.models['LSTM'].eval()
                 # Modo evaluaci√≥n: desactiva dropout (todas las neuronas activas) y fija batchnorm en modo evaluaci√≥n (Usa estad√≠sticas poblacionales (media/var globales)).
    
                with torch.no_grad():
                    # Bloque sin gradientes: as√≠ ahorro memoria y tiempo en validaci√≥n
                    val_pred = self.models['LSTM'](X_val_seq).squeeze()  # Forward en validaci√≥n: misma forma (batch_val,)
                    val_loss = nn.MSELoss()(val_pred, y_val_t) 
                     # P√©rdida de validaci√≥n con MSE: m√©trica consistente con el entrenamiento.
                    
                    if val_loss < best_lstm:
                        # Si mejora la mejor p√©rdida vista, guardo los pesos actuales como "los mejores hasta ahora"
                        best_lstm = val_loss
                        best_state_lstm = self.models['LSTM'].state_dict().copy()
                        # state_dict() = diccionario con todos los tensores de pesos. Hago una copia para congelarlos.

                print(f"   LSTM Epoch {epoch}: {val_loss.item():.4f}")
        
        self.models['LSTM'].load_state_dict(best_state_lstm)
     # Al terminar todas las √©pocas, restauro el MEJOR estado de la LSTM (no el √∫ltimo), lo que mitiga el riesgo de sobreajuste tard√≠o.



        # ------------------------------------------------------------
        #  5) C√ÅLCULO DE ERRORES DE VALIDACI√ìN Y ASIGNACI√ìN DE PESOS
        # ------------------------------------------------------------
        # Una vez entrenados todos los modelos, necesito saber qu√© tan bien lo hace cada uno
        # en el conjunto de validaci√≥n (datos no vistos durante el entrenamiento).
        # Para eso, calculo el error cuadr√°tico medio (MSE) de cada modelo.

        for name, model in self.models.items():
            # Recorro todos los modelos entrenados: Lineal, Ridge, RandomForest, XGBoost, PINN y LSTM.
            if name in ['PINN', 'LSTM']:
                # Las redes neuronales (PINN y LSTM) est√°n en PyTorch y no usan .predict()
                # como los modelos de Scikit-learn, as√≠ que las trato de forma especial.
                model.eval()
                 # Paso a modo evaluaci√≥n:
                 #   - Dropout desactivado
                 #   - BatchNorm fija estad√≠sticas globales
                 # Esto asegura medir el rendimiento real.
                with torch.no_grad():
                     # Desactivo el c√°lculo de gradientes para ahorrar memoria y tiempo,
                     # ya que no necesito entrenar en esta fase, solo evaluar.
                    if name == 'PINN':
                        # La PINN recibe un tensor con forma (batch, features).
                        pred = model(X_val_t).squeeze().numpy()
                    else:
                        # La LSTM requiere secuencias, por eso uso X_val_seq (con dimensi√≥n extra)
                        pred = model(X_val_seq).squeeze().numpy()
            else:
                 # Para los modelos cl√°sicos (Lineal, Ridge, RF, XGBoost),
                 # s√≠ puedo usar directamente el m√©todo .predict() de Scikit-learn/XGBoost.
                pred = model.predict(X_val)
            
            self.val_scores[name] = mean_squared_error(y_val, pred)
            # Guardo el error cuadr√°tico medio (MSE) en un diccionario con el nombre del modelo como clave.
            # Esto me dar√° una medida objetiva de cu√°l modelo predice mejor en validaci√≥n.
        
        # Al terminar el bucle, tengo un diccionario con los MSE de todos los modelos.
        print(f"‚úÖ Validaci√≥n completada! Val MSE: {self.val_scores}")
        # Con estos errores despu√©s calcular√© los pesos del ensemble
        
        
        
 # ------------------------------------------------------------------
# 6) C√ÅLCULO DE PESOS DEL ENSEMBLE (en funci√≥n del error de validaci√≥n)
# ------------------------------------------------------------------
# (ya lo expliqu√© arriba)
# Paso 1: transformo cada MSE en "calidad" usando el inverso 1/MSE. 
#         Si un modelo tiene MSE peque√±o ‚Üí 1/MSE grande ‚Üí m√°s peso.
# Paso 2: normalizo esas calidades para que todos los pesos sumen 1 (distribuci√≥n de probabilidad).
#         As√≠ evito escalas arbitrarias y mantengo interpretabilidad.

        inverse_errors = {k: 1/v for k, v in self.val_scores.items() if v > 0}
        # Diccionario: modelo -> 1/MSE. (Descarto MSE <= 0 por seguridad num√©rica).
        total = sum(inverse_errors.values())
        # Suma total de "calidades" (1/MSE) para normalizar.
        self.weights = {k: v/total for k, v in inverse_errors.items()}
        # Normalizaci√≥n: peso_k = (1/MSE_k) / Œ£_j (1/MSE_j).
# ------------------------------------------------------------------
# 7) PREDICCI√ìN CON EL ENSEMBLE
# ------------------------------------------------------------------
# Procedimiento:    
# Obtengo la predicci√≥n de CADA modelo sobre X_validacion y al final, combino todas las predicciones con los pesos calculados arriba.
#
#  ensemble = Œ£_k (peso_k * pred_k)
#
# detalle: Redes (PyTorch) requieren tensores y modo eval(); modelos sklearn usan .predict()
    def predict(self, X_validacion):
        predictions = {} ## Guardar√° predicciones por modelo: {'Linear': y_hat_lin, 'XGBoost': y_hat_xgb, ...}
        
        for name, model in self.models.items():
            if name in ['PINN', 'LSTM']:
                # Modelos en PyTorch: debo pasar a modo evaluaci√≥n y desactivar gradientes
                model.eval()
                with torch.no_grad():
                    # Convierto el DataFrame a tensor float32
                    X_tensor = torch.FloatTensor(X_validacion.values)
                    if name == 'PINN':
                        # La PINN espera tensores 2D: (batch, features)
                        pred = model(X_tensor).squeeze().numpy()
                    else:
                        # La LSTM espera tensores 3D: (batch, seq_len, features)
                        # Aqu√≠ uso seq_len=1 (ventana temporal de 1 paso)
                        pred = model(X_tensor.unsqueeze(1)).squeeze().numpy()
            else:
                # Modelos cl√°sicos (scikit-learn / XGBoost) usan .predict() directamente sobre DataFrame
                pred = model.predict(X_validacion)
            
            predictions[name] = pred # Guardo la predicci√≥n de cada modelo

        
        # ---------------------------
        # Combinaci√≥n ponderada (Ensemble)
        # ---------------------------
        # Creo un vector de ceros y voy sumando cada predicci√≥n multiplicada por su peso.
        ensemble = np.zeros(len(X_validacion))
        for name, weight in self.weights.items():
            # Sumatorio: y_ensemble = Œ£_k (peso_k * y_pred_k)
            ensemble += weight * predictions[name]
            # Sumatorio: y_ensemble = Œ£_k (peso_k * y_pred_k)
        predictions['Ensemble'] = ensemble
        
        return predictions

# ========================================================================
# EVALUACI√ìN INTEGRAL Y AN√ÅLISIS DE RESULTADOS
# ========================================================================

def evaluate_models(y_true, predictions, title="Evaluation"):
    print(f"\nüìä {title.upper()}")
    print("="*50)
    

    # M√âTRICAS IMPLEMENTADAS:
    
    # 1. R¬≤ (Coeficiente de Determinaci√≥n):
    #   - Rango: (-‚àû, 1], donde 1 = predicci√≥n perfecta
    #   - Interpretaci√≥n: Proporci√≥n de varianza explicada por el modelo
    #   - F√≥rmula: R¬≤ = 1 - (SS_res / SS_tot)
    #   - Ventaja: Normalizado, f√°cil interpretaci√≥n
    #   - Limitaci√≥n: Puede ser enga√±oso con datos no lineales
    
    # 2. RMSE (Root Mean Square Error):
    #   - Unidades: Mismas que la variable objetivo
    #   - Sensible a outliers (penalizaci√≥n cuadr√°tica)
    #   - √ötil para comparar modelos en el mismo dataset
    #   - Interpretaci√≥n directa en t√©rminos f√≠sicos del problema
    
    # 3. MAE (Mean Absolute Error):
    #   - Menos sensible a outliers que RMSE
    #   - M√©trica robusta para evaluaci√≥n general
    #   - Interpretaci√≥n: Error promedio en valor absoluto
    
    # 4. MAPE (Mean Absolute Percentage Error):
    #   - M√©trica relativa independiente de escala
    #   - √ötil para comparar performance entre diferentes datasets
    #   - Limitaci√≥n: Problem√°tico cuando y_true ‚âà 0
    
    # 5. NSE (Nash-Sutcliffe Efficiency):
    #   - M√©trica espec√≠fica para modelado hidrol√≥gico/ambiental
    #   - Compara modelo vs. predicci√≥n con media hist√≥rica
    #   - NSE = 1: predicci√≥n perfecta; NSE = 0: tan bueno como la media
    
    # 6. BIAS (Sesgo Relativo):
    #   - Mide tendencia sistem√°tica del modelo
    #   - Bias > 0: sobreestimaci√≥n; Bias < 0: subestimaci√≥n
    #   - Cr√≠tico para aplicaciones donde la direcci√≥n del error importa
    
    #  DETECCI√ìN DE OVERFITTING:
    #La categorizaci√≥n de riesgo (LOW/MEDIUM/HIGH) se basa en umbrales emp√≠ricos:
    #- R¬≤ > 0.99: ALTO riesgo (posible memorizaci√≥n)
    #- R¬≤ > 0.97: MEDIO riesgo (requiere validaci√≥n adicional)
    #- R¬≤ ‚â§ 0.97: BAJO riesgo (generalizaci√≥n aceptable)

    results = {}
    for name, pred in predictions.items():
        r2 = r2_score(y_true, pred)
        rmse = np.sqrt(mean_squared_error(y_true, pred))
        mae = mean_absolute_error(y_true, pred)
        mape = np.mean(np.abs((y_true - pred) / (y_true + 1e-10))) * 100
        
        # NSE y bias
        nse = 1 - (np.sum((y_true - pred)**2) / np.sum((y_true - np.mean(y_true))**2))
        bias = (np.sum(pred - y_true) / np.sum(y_true)) * 100
        
        # Overfitting 
        risk = "HIGH" if r2 > 0.99 else "MEDIUM" if r2 > 0.97 else "LOW"
        
        results[name] = {
            'R¬≤': r2, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape,
            'NSE': nse, 'Bias': bias, 'Risk': risk
        }
        
        print(f"üî∏ {name.upper()}")
        print(f"   R¬≤={r2:.4f}, RMSE={rmse:.4f}, MAPE={mape:.1f}%")
        print(f"   NSE={nse:.4f}, Bias={bias:.2f}%, Risk={risk}")
    
    return results

# GR√ÅFICOS IMPLEMENTADOS:
    
#    1. SCATTER PLOT (Predicho vs. Observado):
#       - Eval√∫a la correlaci√≥n lineal entre predicciones y valores reales
#       - L√≠nea diagonal roja: predicci√≥n perfecta (y = x)
#       - Desviaciones de la diagonal indican sesgo sistem√°tico
#       - Dispersi√≥n alrededor de la l√≠nea indica precisi√≥n del modelo
    
#    2. BAR PLOT (Comparaci√≥n de R¬≤):
#       - Codificaci√≥n por colores para identificaci√≥n r√°pida de riesgo:
#         * Verde: R¬≤ ‚â§ 0.97 (generalizaci√≥n aceptable)
#         * Naranja: 0.97 < R¬≤ ‚â§ 0.99 (posible overfitting leve)
#         * Rojo: R¬≤ > 0.99 (riesgo alto de overfitting)
    
#    3. RMSE COMPARISON:
#       - M√©trica en unidades originales para interpretaci√≥n pr√°ctica
#       - Permite evaluar significancia pr√°ctica vs. estad√≠stica
    
#    4. AN√ÅLISIS DE RESIDUOS:
#       - Gr√°fico fundamental para validaci√≥n de modelos de regresi√≥n
#       - Patrones en residuos indican violaci√≥n de supuestos:
#         * Heteroscedasticidad: varianza no constante
#         * No linealidad: relaciones no capturadas
#         * Autocorrelaci√≥n: dependencias temporales no modeladas
    
#    5. DISTRIBUCI√ìN DE RESIDUOS:
#       - Test visual de normalidad (supuesto para intervalos de confianza)
#       - Ajuste de curva normal para comparaci√≥n cuantitativa
#       - Desviaciones de normalidad pueden indicar problemas del modelo
    
#    6. COMPARACI√ìN MULTI-M√âTRICA:
#       - Visualizaci√≥n lado a lado de R¬≤ y NSE
#       - Permite evaluaci√≥n hol√≠stica considerando m√∫ltiples criterios
#       - Identifica modelos con rendimiento balanceado vs. especializados

def create_plots(y_true, predictions, results):
    """Create essential publication plots"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # Plot 1: Best model predictions
    best_model = max(results.keys(), key=lambda k: results[k]['R¬≤'])
    pred_best = predictions[best_model]
    
    axes[0,0].scatter(y_true, pred_best, alpha=0.6, s=20)
    min_val, max_val = min(y_true.min(), pred_best.min()), max(y_true.max(), pred_best.max())
    axes[0,0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)
    axes[0,0].set_xlabel('Observed (g/L)')
    axes[0,0].set_ylabel('Predicted (g/L)')
    axes[0,0].set_title(f'{best_model} - R¬≤={results[best_model]["R¬≤"]:.3f}')
    axes[0,0].grid(True, alpha=0.3)
    
    # Plot 2: R¬≤ comparison
    models = list(results.keys())
    r2_vals = [results[m]['R¬≤'] for m in models]
    bars = axes[0,1].bar(models, r2_vals)
    for bar, r2 in zip(bars, r2_vals):
        bar.set_color('red' if r2 > 0.99 else 'orange' if r2 > 0.97 else 'green')
    axes[0,1].set_ylabel('R¬≤ Score')
    axes[0,1].set_title('Model Comparison')
    axes[0,1].tick_params(axis='x', rotation=45)
    axes[0,1].grid(True, alpha=0.3)
    
    # Plot 3: RMSE comparison
    rmse_vals = [results[m]['RMSE'] for m in models]
    axes[0,2].bar(models, rmse_vals)
    axes[0,2].set_ylabel('RMSE (g/L)')
    axes[0,2].set_title('RMSE Comparison')
    axes[0,2].tick_params(axis='x', rotation=45)
    axes[0,2].grid(True, alpha=0.3)
    
    # Plot 4: Residuals
    residuals = y_true - pred_best
    axes[1,0].scatter(pred_best, residuals, alpha=0.6, s=20)
    axes[1,0].axhline(0, color='red', linestyle='--')
    axes[1,0].set_xlabel('Predictions')
    axes[1,0].set_ylabel('Residuals')
    axes[1,0].set_title('Residual Analysis')
    axes[1,0].grid(True, alpha=0.3)
    
    # Plot 5: Residual distribution
    axes[1,1].hist(residuals, bins=30, density=True, alpha=0.7)
    mu, sigma = stats.norm.fit(residuals)
    x = np.linspace(residuals.min(), residuals.max(), 100)
    axes[1,1].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', lw=2)
    axes[1,1].set_xlabel('Residuals')
    axes[1,1].set_ylabel('Density')
    axes[1,1].set_title('Residual Distribution')
    axes[1,1].grid(True, alpha=0.3)
    
    # Plot 6: Multi-metric radar (simplified)
    metrics = ['R¬≤', 'NSE']
    model_names = list(results.keys())[:4]  # Top 4 models
    
    x = np.arange(len(model_names))
    width = 0.35
    
    r2_vals = [results[m]['R¬≤'] for m in model_names]
    nse_vals = [results[m]['NSE'] for m in model_names]
    
    axes[1,2].bar(x - width/2, r2_vals, width, label='R¬≤', alpha=0.8)
    axes[1,2].bar(x + width/2, nse_vals, width, label='NSE', alpha=0.8)
    axes[1,2].set_xlabel('Models')
    axes[1,2].set_ylabel('Score')
    axes[1,2].set_title('Multi-Metric Comparison')
    axes[1,2].set_xticks(x)
    axes[1,2].set_xticklabels(model_names, rotation=45)
    axes[1,2].legend()
    axes[1,2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

#  AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS
    
#    1. INTERPRETABILIDAD DEL MODELO:
#       - Identificar qu√© variables son m√°s relevantes para las predicciones
#       - Facilitar la comprensi√≥n del modelo por parte de expertos del dominio
#       - Cumplir con requisitos de explicabilidad en aplicaciones cr√≠ticas
    
#    2. REDUCCI√ìN DIMENSIONAL:
#       - Identificar caracter√≠sticas redundantes o irrelevantes
#       - Optimizar el modelo eliminando features de baja importancia
#       - Reducir complejidad computacional y riesgo de overfitting
    
#    3. VALIDACI√ìN CIENT√çFICA:
#       - Verificar que el modelo identifica relaciones conocidas del dominio
#       - Descubrir nuevas relaciones potencialmente importantes
#       - Contrastar resultados con conocimiento experto previo
    
#    4. FEATURE ENGINEERING:
#       - Guiar la creaci√≥n de nuevas caracter√≠sticas derivadas
#       - Informar decisiones sobre transformaciones de variables
#       - Identificar interacciones importantes entre variables
    
#    M√âTODOS DE IMPORTANCIA:
#    - Tree-based models: Importancia basada en reducci√≥n de impureza (Gini, entropy)
#    - Linear models: Coeficientes normalizados o m√©todos de permutaci√≥n
#    - Neural networks: Gradientes, saliency maps, SHAP values
def analyze_importance(model, feature_names):
    """Feature importance analysis"""
    if hasattr(model, 'feature_importances_'):
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print("\nüîç TOP 10 FEATURES:")
        for _, row in importance_df.head(10).iterrows():
            print(f"   {row['feature']}: {row['importance']:.4f}")
        
        return importance_df
    return None

# ========================================================================
# MAIN PIPELINE
# ========================================================================

def run_compact_research():
    
    # 1. CARGA Y LIMPIA LOS DATOS
    manager = SmartDataManager()
    data, leakage = manager.carga_limpia_datos()
    # Se cargan los datos originales y se hace una limpieza inicial.
    # Tambi√©n se identifican y eliminan variables con posible "data leakage"
    # (features que contienen informaci√≥n del futuro o de la variable objetivo
    # y que falsear√≠an el entrenamiento si se incluyeran).
    
    # 2. FEATURE ENGINEERING
    engine = BioFeatureEngine()
    df_features = engine.create_features(data)
    # Se construyen nuevas variables derivadas (ej. interacciones, normalizaciones,
    # indicadores biol√≥gicos) a partir de los datos originales.
    # El objetivo es enriquecer el dataset con informaci√≥n m√°s expresiva
    # para mejorar la capacidad predictiva de los modelos.

    # 3. SEPARAR LOS DATOS
    X_entrenamiento_s, X_validacion_s, Y_entrenamiento_s, Y_validacion_s, Y_entrenamiento_orig, Y_validacion_orig = engine.seleccion_y_preparacion_features(df_features)
    # Se dividen los datos en conjunto de entrenamiento (para ajustar los modelos)
    # y conjunto de validaci√≥n (para evaluar su desempe√±o en datos no vistos).
    # "_s" indica que las variables han sido escaladas (normalizadas).
    # Tambi√©n se guardan los valores originales (sin escalar) de Y para poder
    # interpretar los resultados en unidades reales m√°s adelante.

    # 4. ENTRENAR LOS MODELOS
    system = CompactMultiModel()
    system.train_all(X_entrenamiento_s, Y_entrenamiento_s)
    # Se inicializa el sistema que contiene todos los modelos (lineales, √°rboles,
    # XGBoost, PINN, LSTM, etc.) y se entrena cada uno sobre los datos escalados.
    # Cada modelo aprende relaciones distintas, y despu√©s se combinar√°n en un ensemble.

    # 5. OBTENER PREDICCIONES
    predictions_scaled = system.predict(X_validacion_s)
    # Se obtienen predicciones de todos los modelos, pero todav√≠a en la escala
    # normalizada usada durante el entrenamiento (ej. entre 0 y 1).

    
    # 6. DESNORMALIZACI√ìN (INTERPRETABILIDAD)
    # Se transforman las predicciones a la escala original (g/mol o g/L),
    # para que tengan un significado f√≠sico y se puedan comparar con las observaciones reales.
    predictions_orig = {}
    for name, pred_scaled in predictions_scaled.items():
        pred_orig = engine.scalers['target'].inverse_transform(pred_scaled.reshape(-1, 1)).flatten()
        # con esto obtengo la predicci√≥n final de nuevo en g_mol para poder interpretarla de manera correcta.
        predictions_orig[name] = pred_orig
    
    # 7. EVALUACI√ìN FINAL DE MODELOS
    results = evaluate_models(Y_validacion_orig, predictions_orig, "COMPACT SYSTEM RESULTS")
    # Se eval√∫an todos los modelos usando varias m√©tricas (R¬≤, RMSE, MAE, MAPE, NSE, Bias).
    # Esto permite un diagn√≥stico completo: ajuste general, error absoluto, error relativo,
    # sesgos sistem√°ticos y riesgo de sobreajuste.
    
    # 8. VISUALIZACI√ìN
    create_plots(Y_validacion_orig, predictions_orig, results)
    # Se generan gr√°ficos de apoyo para comunicar resultados:
    #   - Diagrama de paridad observado vs predicho.
    #   - Comparaciones de R¬≤ y RMSE entre modelos.
    #   - An√°lisis de residuos y distribuci√≥n.
    #   - Comparativa multi-m√©trica.
    # Estos gr√°ficos son m√°s comunicativos que las m√©tricas num√©ricas aisladas.

    
    # 9. FEATURES IMPORTANTES
    if 'XGBoost' in system.models:
        importance = analyze_importance(system.models['XGBoost'], engine.selected_features)
    # Para modelos que lo permiten (ej. RandomForest, XGBoost), se analiza la
    # importancia relativa de cada variable de entrada.
    # Esto aporta interpretabilidad: ¬øqu√© factores influyen m√°s en la predicci√≥n de biomasa?

    
    # 10. MEJOR MODELO, R2 Y RMSE
    best_model = max(results.keys(), key=lambda k: results[k]['R¬≤'])
    best_r2 = results[best_model]['R¬≤']
    best_rmse = results[best_model]['RMSE']
    # Se selecciona autom√°ticamente el modelo con mejor R¬≤ en validaci√≥n.
    # Tambi√©n se guardan sus m√©tricas clave (R¬≤ y RMSE).

    
    print(f"\nüéâ MODELO PREDICTIVO COMPLETADO!")
    print("="*40)
    print(f"üèÜ Mejor Modelo: {best_model}")
    print(f"üìä Performance: R¬≤={best_r2:.4f}, RMSE={best_rmse:.4f}")
    print(f"üõ°Ô∏è Anti-overfitting: {results[best_model]['Risk']} risk")
    print(f"üìà Leakage eliminado: {len(leakage)} features")
    print(f"üî¨ Features utilizadas: {len(engine.selected_features)}")
    
    return {
        'system': system, 'results': results, 'engine': engine,
        'best_model': best_model, 'leakage_removed': len(leakage)
    }

# ========================================================================
# EJECUCI√ìN
# ========================================================================

if __name__ == "__main__":
    
    final_results = run_compact_research()
    
    if final_results:
        print(f"\n‚úÖ SUCCESS!")
